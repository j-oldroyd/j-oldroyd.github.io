<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*       on 2022-03-24T14:32:07-04:00       *-->
<!--*   A recent stable commit (2020-08-09):   *-->
<!--* 98f21740783f166a773df4dc83cab5293ab63a4a *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US">
<head xmlns:og="http://ogp.me/ns#" xmlns:book="https://ogp.me/ns/book#">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>WVWC-AEM Diagonalization of Matrices</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:type" content="book">
<meta property="book:title" content="Advanced Engineering Mathematics Lecture Notes">
<meta property="book:author" content="Jesse Oldroyd">
<script src="https://sagecell.sagemath.org/embedded_sagecell.js"></script><script>window.MathJax = {
  tex: {
    inlineMath: [['\\(','\\)']],
    tags: "none",
    useLabelIds: true,
    tagSide: "right",
    tagIndent: ".8em",
    packages: {'[+]': ['base', 'extpfeil', 'ams', 'amscd', 'newcommand', 'knowl']}
  },
  options: {
    ignoreHtmlClass: "tex2jax_ignore|ignore-math",
    processHtmlClass: "process-math",
    renderActions: {
        findScript: [10, function (doc) {
            document.querySelectorAll('script[type^="math/tex"]').forEach(function(node) {
                var display = !!node.type.match(/; *mode=display/);
                var math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                var text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = {node: text, delim: '', n: 0};
                math.end = {node: text, delim: '', n: 0};
                doc.math.push(math);
            });
        }, '']
    },
  },
  chtml: {
    scale: 0.88,
    mtextInheritFont: true
  },
  loader: {
    load: ['input/asciimath', '[tex]/extpfeil', '[tex]/amscd', '[tex]/newcommand', '[pretext]/mathjaxknowl3.js'],
    paths: {pretext: "https://pretextbook.org/js/lib"},
  },
};
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><script>$(function () {
    // Make *any* div with class 'sagecell-sage' an executable Sage cell
    // Their results will be linked, only within language type
    sagecell.makeSagecell({inputLocation: 'div.sagecell-sage',
                           linked: true,
                           languages: ['sage'],
                           evalButtonText: 'Evaluate (Sage)'});
});
</script><script>$(function () {
    // Make *any* div with class 'sagecell-octave' an executable Sage cell
    // Their results will be linked, only within language type
    sagecell.makeSagecell({inputLocation: 'div.sagecell-octave',
                           linked: true,
                           languages: ['octave'],
                           evalButtonText: 'Evaluate (Octave)'});
});
</script><script src="https://pretextbook.org/js/lib/jquery.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.13/pretext.js"></script><script>miniversion=0.674</script><script src="https://pretextbook.org/js/0.13/pretext_add_on.js?x=1"></script><script src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script>sagecellEvalName='Evaluate (Sage)';
</script><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<script src="https://cdn.geogebra.org/apps/deployggb.js"></script><link href="https://pretextbook.org/css/0.4/pretext.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/banner_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/toc_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/knowls_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/style_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/colors_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/setcolors.css" rel="stylesheet" type="text/css">
<!-- 2019-10-12: Temporary - CSS file for experiments with styling --><link href="developer.css" rel="stylesheet" type="text/css">
</head>
<body class="pretext-book ignore-math has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div id="latex-macros" class="hidden-content process-math" style="display:none"><span class="process-math">\(\require{physics}\newcommand{\RR}{\mathbb{R}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\CC}{\mathbb{C}}
\renewcommand{\th}{\text{th}}
\newcommand{\xx}{\mathbf{x}}
\newcommand{\yy}{\mathbf{y}}
\newcommand{\zz}{\mathbf{z}}
\newcommand{\ii}{\mathbf{i}}
\newcommand{\jj}{\mathbf{j}}
\newcommand{\kk}{\mathbf{k}}
\newcommand{\uu}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\ee}{\mathbf{e}}
\newcommand{\rr}{\mathbf{r}}
\newcommand{\rrp}{\mathbf{r}^\prime}
\newcommand{\TT}{\mathbf{T}}
\newcommand{\col}[1]{\operatorname{col}{#1}}
\newcommand{\dotprod}[1]{\left\langle #1 \right\rangle}
\newcommand{\nul}[1]{\operatorname{null}{#1}}
\newcommand{\spn}[1]{\operatorname{span}{#1}}
\newcommand{\rowop}[2][]{\overset{#2}{\underset{#1}{\sim}}}
\newcommand{\Sum}[2]{\sum_{#1}^{#2}}
\newcommand{\Int}[2]{\int_{#1}^{#2}}
\newcommand{\limit}[2]{\lim_{#1\to#2}}
\newcommand{\Laplace}[1]{\mathcal{L}\left\{#1\right\}}
\newcommand{\iLaplace}[1]{\mathcal{L}^{-1}\left\{#1\right\}}
\renewcommand{\vecm}[1]{\boldsymbol{#1}}
\newcommand{\ivec}[1]{
\renewcommand{\arraystretch}{.8}
\begin{bmatrix}#1\end{bmatrix}
      }
\newcommand{\proj}[2]{\operatorname{proj}_{#1} #2}
\providecommand{\rank}[1]{\operatorname{rank}{#1}}
\providecommand{\grad}{\nabla}
\providecommand{\dv}[3][]{\dfrac{d^{#1} #2}{d #3^{#1}}}
\providecommand{\pdv}[3][]{\dfrac{\partial^{#1} #2}{\partial #3^{#1}}}
\providecommand{\dd}[2][]{\, d^{#1} #2\ }
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\)</span></div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href="" target="_blank"><img src="external/./wvwc-logo.jpg" alt="Logo image"></a><div class="title-container">
<h1 class="heading"><a href="wvwc-advanced-engineering-math.html"><span class="title">Advanced Engineering Mathematics Lecture Notes:</span> <span class="subtitle">West Virginia Wesleyan College</span></a></h1>
<p class="byline">Jesse Oldroyd</p>
</div>
</div></div>
<nav id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3">
<a class="index-button toolbar-item button" href="index-1.html" title="Index">Index</a><button id="calculator-toggle" class="toolbar-item button toggle" title="Show calculator" aria-expanded="false" aria-controls="calculator-container">Calc</button><div id="calculator-container" class="calculator-container" style="display: none; z-index:100;"><div id="geogebra-calculator"></div></div>
<script>
var ggbApp = new GGBApplet({"appName": "graphing",
    "width": 330,
    "height": 600,
    "showToolBar": true,
    "showAlgebraInput": true,
    "perspective": "G/A",
    "algebraInputPosition": "bottom",
    "scaleContainerClass": "calculator-container",
    "allowUpscale": true,
    "autoHeight": true,
    "disableAutoScale": false},
true);
</script><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="section-orthogonal-transformations.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="chapter-linear-eigenvalues-eigenvectors.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="part-multivariable-calculus.html" title="Next">Next</a></span>
</div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="section-orthogonal-transformations.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="chapter-linear-eigenvalues-eigenvectors.html" title="Up">Up</a><a class="next-button button toolbar-item" href="part-multivariable-calculus.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link frontmatter">
<a href="frontmatter.html" data-scroll="frontmatter" class="internal"><span class="title">Front Matter</span></a><ul>
<li><a href="front-colophon.html" data-scroll="front-colophon" class="internal">Colophon</a></li>
<li><a href="acknowledgement.html" data-scroll="acknowledgement" class="internal">Acknowledgements</a></li>
<li><a href="preface.html" data-scroll="preface" class="internal">Preface</a></li>
</ul>
</li>
<li class="link part"><a href="part-linear-algebra.html" data-scroll="part-linear-algebra" class="internal"><span class="codenumber">I</span> <span class="title">Linear Algebra</span></a></li>
<li class="link">
<a href="chapter-linear-algebra-matrices.html" data-scroll="chapter-linear-algebra-matrices" class="internal"><span class="codenumber">1</span> <span class="title">Introduction to Matrix Algebra</span></a><ul>
<li><a href="section-matrices-vectors-and-linear-combinations.html" data-scroll="section-matrices-vectors-and-linear-combinations" class="internal">Matrices, Vectors and Linear Combinations</a></li>
<li><a href="section-matrix-multiplication.html" data-scroll="section-matrix-multiplication" class="internal">Matrix Multiplication</a></li>
<li><a href="section-systems-of-linear-equations.html" data-scroll="section-systems-of-linear-equations" class="internal">Systems of Linear Equations</a></li>
<li><a href="section-linear-independence.html" data-scroll="section-linear-independence" class="internal">Linear Independence</a></li>
<li><a href="section-existence-of-solutions.html" data-scroll="section-existence-of-solutions" class="internal">Existence of Solutions</a></li>
<li><a href="section-determinants.html" data-scroll="section-determinants" class="internal">Determinants</a></li>
<li><a href="section-matrix-inverses.html" data-scroll="section-matrix-inverses" class="internal">Matrix Inverses</a></li>
<li><a href="section--LU-decomposition.html" data-scroll="section--LU-decomposition" class="internal"><span class="process-math">\(LU\)</span> Decomposition</a></li>
</ul>
</li>
<li class="link">
<a href="chapter-linear-eigenvalues-eigenvectors.html" data-scroll="chapter-linear-eigenvalues-eigenvectors" class="internal"><span class="codenumber">2</span> <span class="title">Eigenvalues and Eigenvectors</span></a><ul>
<li><a href="section-finding-eigenvalues-and-eigenvectors.html" data-scroll="section-finding-eigenvalues-and-eigenvectors" class="internal">Finding Eigenvalues and Eigenvectors</a></li>
<li><a href="section-eigenvalue-problems.html" data-scroll="section-eigenvalue-problems" class="internal">Eigenvalue Problems</a></li>
<li><a href="section-orthogonal-transformations.html" data-scroll="section-orthogonal-transformations" class="internal">Orthogonal Transformations</a></li>
<li><a href="section-diagonalization-of-matrices.html" data-scroll="section-diagonalization-of-matrices" class="active">Diagonalization of Matrices</a></li>
</ul>
</li>
<li class="link part"><a href="part-multivariable-calculus.html" data-scroll="part-multivariable-calculus" class="internal"><span class="codenumber">II</span> <span class="title">Multivariable Calculus</span></a></li>
<li class="link">
<a href="chapter-multivariable-calculus-vector-derivatives.html" data-scroll="chapter-multivariable-calculus-vector-derivatives" class="internal"><span class="codenumber">3</span> <span class="title">Vector Derivatives</span></a><ul>
<li><a href="section-inner-products-and-cross-products.html" data-scroll="section-inner-products-and-cross-products" class="internal">Inner Products and Cross Products</a></li>
<li><a href="section-vector-functions.html" data-scroll="section-vector-functions" class="internal">Vector Functions</a></li>
</ul>
</li>
<li class="link backmatter"><a href="backmatter.html" data-scroll="backmatter" class="internal"><span class="title">Back Matter</span></a></li>
<li class="link"><a href="appendix-gfdl.html" data-scroll="appendix-gfdl" class="internal"><span class="codenumber">A</span> <span class="title">GNU Free Documentation License</span></a></li>
<li class="link"><a href="index-1.html" data-scroll="index-1" class="internal"><span class="title">Index</span></a></li>
<li class="link"><a href="colophon-2.html" data-scroll="colophon-2" class="internal"><span class="title">Colophon</span></a></li>
</ul></nav><div class="extras"><nav><a class="pretext-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content">
<section class="section" id="section-diagonalization-of-matrices"><h2 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber">2.4</span> <span class="title">Diagonalization of Matrices</span>
</h2>
<section class="introduction" id="introduction-1"><p id="p-188">In this section we consider bases of <span class="process-math">\(\mathbb{R}^n\)</span> that are associated with eigenvectors a square matrix <span class="process-math">\(A\)</span> known as eigenbases (see <a href="" class="xref" data-knowl="./knowl/definition-eigenbases.html" title="Definition 2.1.14: Eigenbases">Definition 2.1.14</a>). The benefit to looking at such a basis instead of using the standard basis of <span class="process-math">\(\RR^n\)</span> is that the eigenbasis will make products involving <span class="process-math">\(A\)</span> much simpler through a process known as <em class="emphasis">diagonalization</em>.</p></section><section class="subsection" id="subsection-eigenbases"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber"></span> <span class="title">Eigenbases</span>
</h3>
<p id="p-189">Recall that a basis of <span class="process-math">\(\RR^n\)</span> is a linearly independent collection of <span class="process-math">\(n\)</span> vectors in <span class="process-math">\(\RR^n\)</span> (see also <a href="" class="xref" data-knowl="./knowl/definition-basis-of--rr-n-.html" title="Definition 1.4.1: Basis of \RR^n">Definition 1.4.1</a>). The defining characteristic of a basis is this: if <span class="process-math">\(\qty{\vb{b}_1,\ldots,\vb{b}_n}\)</span> is a basis of <span class="process-math">\(\RR^n\)</span> and if <span class="process-math">\(\vb{x}\in\RR^n\text{,}\)</span> then there exists a unique set of scalars <span class="process-math">\(c_1,\ldots,c_n\)</span> such that</p>
<div class="displaymath process-math">
\begin{equation*}
\xx = \sum_{i=1}^{n}c_i\vb{b}_i\text{.}
\end{equation*}
</div>
<p class="continuation">This makes it possible to use the basis as a coordinate system in <span class="process-math">\(\RR^n\text{.}\)</span> Therefore we may view an eigenbasis (<a href="" class="xref" data-knowl="./knowl/definition-eigenbases.html" title="Definition 2.1.14: Eigenbases">Definition 2.1.14</a>) of a matrix <span class="process-math">\(A\)</span> as a particular coordinate system that is well-suited to calculations involving <span class="process-math">\(A\text{,}\)</span> an idea which we make precise below.</p>
<article class="example example-like" id="example-using-an-eigenbasis-to-compute-a-matrix-product"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">2.4.1</span><span class="period">.</span><span class="space"> </span><span class="title">Using an Eigenbasis to Compute a Matrix Product.</span>
</h4>
<p id="p-190">Let</p>
<div class="displaymath process-math">
\begin{equation*}
A = \mqty[-2 &amp; 3 \\ -4 &amp; 5]\text{ and }\vb{b} = \mqty[-2 \\ 10]\text{.}
\end{equation*}
</div>
<p class="continuation">Given that</p>
<div class="displaymath process-math">
\begin{equation*}
\vv_1 = \mqty[1\\1]\text{ and }\vv_2 = \mqty[3\\4]
\end{equation*}
</div>
<p class="continuation">are eigenvectors of <span class="process-math">\(A\)</span> with corresponding eigenvalues <span class="process-math">\(\lambda_1 = 1\)</span> and <span class="process-math">\(\lambda_2 = 2\text{,}\)</span> find <span class="process-math">\(A^{100}\bb\text{.}\)</span></p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-21" id="solution-21"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-21"><div class="solution solution-like">
<p id="p-191">First, note that <span class="process-math">\(\qty{\vv_1,\vv_2}\)</span> is a basis of <span class="process-math">\(\RR^2\text{.}\)</span> Therefore it's an eigenbasis since each vector is an eigenvector of <span class="process-math">\(A\text{.}\)</span> <aside class="aside aside-like" id="aside-3"><h5 class="heading"><span class="title">Bases in <span class="process-math">\(\RR^2\)</span>.</span></h5>
<p id="p-192">One way to see that <span class="process-math">\(\qty{\vv_1,\vv_2}\)</span> must be a basis of <span class="process-math">\(\RR^2\)</span> is to observe that <span class="process-math">\(\vv_2\)</span> is not a scalar multiple of <span class="process-math">\(\vv_1\)</span> and so the two vectors are linearly independent. Then <span class="process-math">\(\qty{\vv_1,\vv_2}\)</span> is a set of two linearly independent vectors in the two-dimensional vector space <span class="process-math">\(\RR^2\text{,}\)</span> and so this set must be a basis of <span class="process-math">\(\RR^2\text{.}\)</span></p></aside></p>
<p id="p-193">Since <span class="process-math">\(\qty{\vv_1,\vv_2}\)</span> is a basis of <span class="process-math">\(\RR^2\)</span> then there exist scalars <span class="process-math">\(c_1,c_2\)</span> such that <span class="process-math">\(\bb = c_1\vv_1 + c_2\vv_2\text{.}\)</span> We can find these scalars by row reduction of the augmented matrix <span class="process-math">\(\mqty[\vv_1 &amp; \vv_2 &amp; \bb]\text{.}\)</span> This reduces to</p>
<div class="displaymath process-math">
\begin{equation*}
\mqty[1 &amp; 0 &amp; -38 \\ 0 &amp; 1 &amp; 12]\text{,}
\end{equation*}
</div>
<p class="continuation">and so <span class="process-math">\(c_1 = -38, c_2 = 12\)</span> and</p>
<div class="displaymath process-math">
\begin{equation*}
\bb = -38\vv_1 + 12\vv_2\text{.}
\end{equation*}
</div>
<p id="p-194">Now that we've written <span class="process-math">\(\bb\)</span> in terms of the eigenbasis <span class="process-math">\(\qty{\vv_1,\vv_2}\text{,}\)</span> the computation of <span class="process-math">\(A^{100}\bb\)</span> becomes almost trivial:</p>
<div class="displaymath process-math">
\begin{align*}
A^{100}\bb &amp; = A^{100}(-38\vv_1 + 12\vv_2) \\
&amp; = -38 A^{100}\vv_1 + 12 A^{100}\vv_2 \\
&amp; = -38 \vv_1 + 12\cdot 2^{100}\vv_2 \\
&amp; = \mqty[-38 + 36(2^{100}) \\ -38 + 48(2^{100})] \text{.}
\end{align*}
</div>
<p class="continuation"><aside class="aside aside-like" id="aside-4"><p id="p-195">The second to last line of this computation makes use of the fact that</p>
<div class="displaymath process-math">
\begin{equation*}
A^n\xx = \lambda^n\xx
\end{equation*}
</div>
<p class="continuation">for any eigenvector <span class="process-math">\(\xx\)</span> of <span class="process-math">\(A\)</span> with eigenvalue <span class="process-math">\(\lambda\text{.}\)</span></p></aside></p>
</div></div>
</div></article><div class="sagecell-octave" id="sage-14"><script type="text/x-sage">format short
A = [1, 3, -2; 1, 4, 10]
rref(A) # reduces to find c_1, c_2
</script></div>
<p id="p-196"><a href="" class="xref" data-knowl="./knowl/example-using-an-eigenbasis-to-compute-a-matrix-product.html" title="Example 2.4.1: Using an Eigenbasis to Compute a Matrix Product">Example 2.4.1</a> shows that the existence of an eigenbasis can greatly simplify certain computations. Unfortunately, not every matrix has a corresponding eigenbasis (see <a href="" class="xref" data-knowl="./knowl/definition-defective-matrices.html" title="Definition 2.1.12: Defective Matrices">Definition 2.1.12</a>). However, the following theorem gives a simple condition that can be used to guarantee the existence of an eigenbasis.</p>
<article class="theorem theorem-like" id="theorem-distinct-eigenvalues-and-eigenbases"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">2.4.2</span><span class="period">.</span><span class="space"> </span><span class="title">Distinct Eigenvalues and Eigenbases.</span>
</h4>
<p id="p-197">Let <span class="process-math">\(A\)</span> be an <span class="process-math">\(n\times n\)</span> matrix and suppose that <span class="process-math">\(A\)</span> has <span class="process-math">\(n\)</span> distinct eigenvalues (equivalently, no eigenvalue is repeated). Then <span class="process-math">\(A\)</span> has an eigenbasis.</p></article><article class="hiddenproof" id="proof-5"><a href="" data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-5"><h4 class="heading"><span class="type">Proof<span class="period">.</span></span></h4></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-5"><article class="hiddenproof"><p id="p-198">The proof of this statement follows from the fact that eigenvectors corresponding to distinct eigenvalues must be linearly independent, so we'll prove this first. So let <span class="process-math">\(\qty{\lambda_i}_{i=1}^n\)</span> denote the eigenvalues of <span class="process-math">\(A\)</span> and let <span class="process-math">\(\xx_i\)</span> denote an eigenvector of <span class="process-math">\(A\)</span> corresponding to <span class="process-math">\(\lambda_i\text{.}\)</span> We'll show that <span class="process-math">\(\qty{\xx_i}_{i=1}^n\)</span> is a linearly independent set. As this will then be a set of <span class="process-math">\(n\)</span> linearly independent vectors in <span class="process-math">\(\RR^n\text{,}\)</span> this is enough to show that it's a basis as well.</p>
<p id="p-199">Suppose that we have scalars <span class="process-math">\(\qty{c_i}\)</span> such that <span class="process-math">\(\sum_{i=1}^n c_i\xx_i = \vb{0}\text{.}\)</span> We need to show that <span class="process-math">\(c_1=\ldots=c_n=0\text{.}\)</span> Now, since each <span class="process-math">\(\xx_i\)</span> is an eigenvector of <span class="process-math">\(A\)</span> with eigenvalue <span class="process-math">\(\lambda_i\text{,}\)</span> it follows that</p>
<div class="displaymath process-math">
\begin{equation*}
A\qty(\sum_{i=1}^n c_i\xx_i) = \sum_{i=1}^n c_i A\xx_i = \sum_{i=1}^n c_i\lambda_i\xx_i\text{.}
\end{equation*}
</div>
<p class="continuation">Since <span class="process-math">\(A\vb{0} = \vb{0}\)</span> as well, we have</p>
<div class="displaymath process-math">
\begin{equation*}
\sum_{i=1}^n c_i\lambda_i\xx_i = \vb{0}\text{.}
\end{equation*}
</div>
<p class="continuation">We can also multiply the original equation <span class="process-math">\(\sum_{i=1}^{n}c_i\xx_i = \vb{0}\)</span> by <span class="process-math">\(\lambda_1\)</span> to get</p>
<div class="displaymath process-math">
\begin{equation*}
\sum_{i=1}^{n}\lambda_1 c_i\xx_i = \vb{0}\text{.}
\end{equation*}
</div>
<p id="p-200">Subtracting the previous two equations allows us to write</p>
<div class="displaymath process-math">
\begin{equation*}
\vb{0} = \sum_{i=1}^n c_i (\lambda_i - \lambda_1)\xx_i = \sum_{i=2}^n c_i(\lambda_i-\lambda_1)\xx_i = \sum_{i=2}^{n}d_i \xx_i
\end{equation*}
</div>
<p class="continuation">where <span class="process-math">\(d_i = c_i(\lambda_i-\lambda_1)\text{.}\)</span> Now we can repeat the above process and write</p>
<div class="displaymath process-math">
\begin{equation*}
\sum_{i=2}^n d_i\lambda_i\xx_i = \vb{0} = \sum_{i=2}^n d_i\lambda_2\xx_i
\end{equation*}
</div>
<p class="continuation">which gives (after subtracting)</p>
<div class="displaymath process-math">
\begin{equation*}
\vb{0} = \sum_{i=2}^n d_i (\lambda_i-\lambda_2)\xx_i = \sum_{i=3}^n d_i (\lambda_i-\lambda_2)\xx_i = \sum_{i=3}^n e_i\xx_i
\end{equation*}
</div>
<p class="continuation">where <span class="process-math">\(e_i = (\lambda_i-\lambda_2)d_i = (\lambda_i-\lambda_2)(\lambda_i-\lambda_1)c_i\text{.}\)</span> Continuing this process, we are left with the equation</p>
<div class="displaymath process-math">
\begin{equation*}
\vb{0} = (\lambda_n-\lambda_{n-1})(\lambda_n-\lambda_{n-2})\cdots(\lambda_n-\lambda_1)c_n\xx_n\text{,}
\end{equation*}
</div>
<p class="continuation">which forces <span class="process-math">\(c_n = 0\text{.}\)</span></p>
<p id="p-201">Now we are left with</p>
<div class="displaymath process-math">
\begin{equation*}
\sum_{i=1}^{n-1}c_{i}\xx_i = \vb{0}
\end{equation*}
</div>
<p class="continuation">since we can safely disregard <span class="process-math">\(c_n\xx_n\text{.}\)</span> But there's nothing stopping us from applying the previous trick to this new sum, which will eventually show that <span class="process-math">\(c_{n-1} = 0\text{,}\)</span> and then <span class="process-math">\(c_{n-2} = 0\text{,}\)</span> and so on. Therefore</p>
<div class="displaymath process-math">
\begin{equation*}
c_1=\ldots = c_n = 0
\end{equation*}
</div>
<p class="continuation">and the set <span class="process-math">\(\qty{\xx_i}_{i=1}^n\)</span> must be linearly independent, which was what we needed to prove.</p></article></div></section><section class="subsection" id="subsection-diagonalization"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber"></span> <span class="title">Diagonalization</span>
</h3>
<p id="p-202">Now we'll take a closer look at just what we did in <a href="" class="xref" data-knowl="./knowl/example-using-an-eigenbasis-to-compute-a-matrix-product.html" title="Example 2.4.1: Using an Eigenbasis to Compute a Matrix Product">Example 2.4.1</a> to compute <span class="process-math">\(A^{100}\bb\)</span> (or, more simply, <span class="process-math">\(A\bb\)</span>). First, we found <span class="process-math">\(c_1,c_2\)</span> such that <span class="process-math">\(\bb = c_1\vv_1 + c_2\vv_2\text{.}\)</span> If we let <span class="process-math">\(P = \smqty[\vv_1&amp;\vv_2]\)</span> then this is equivalent to solving the matrix equation</p>
<div class="displaymath process-math">
\begin{equation*}
P\mqty[c_1 \\ c_2] = \bb\implies \mqty[c_1 \\ c_2] = P^{-1}\bb\text{.}
\end{equation*}
</div>
<p class="continuation">We therefore view <span class="process-math">\(P^{-1}\bb\)</span> as the coordinates of <span class="process-math">\(\bb\)</span> with respect to the eigenbasis <span class="process-math">\(\qty{\vv_1,\vv_2}\text{.}\)</span> Once we had the coordinates of <span class="process-math">\(\bb\)</span> with respect to the eigenbasis, then finding <span class="process-math">\(A\bb\)</span> amounted to multiplying <span class="process-math">\(c_1\)</span> and <span class="process-math">\(c_2\)</span> by <span class="process-math">\(\lambda_1\)</span> and <span class="process-math">\(\lambda_2\)</span> respectively. In matrix notation, this is equivalent to computing</p>
<div class="displaymath process-math">
\begin{equation*}
D\mqty[c_1 \\ c_2]\text{ where } D = \mqty[\lambda_1 &amp; 0 \\ 0 &amp; \lambda_2]\text{.}
\end{equation*}
</div>
<p class="continuation">Finally, we used the weights <span class="process-math">\(\lambda_1 c_1, \lambda_2 c_2\)</span> to reconstruct <span class="process-math">\(A\bb\)</span> from <span class="process-math">\(\vv_1,\vv_2\text{:}\)</span></p>
<div class="displaymath process-math">
\begin{equation*}
A\bb = \lambda_1 c_1\vv_1 + \lambda_2 c_2\vv_2\text{.}
\end{equation*}
</div>
<p class="continuation">Therefore</p>
<div class="displaymath process-math">
\begin{align*}
A\bb &amp; = \lambda_1 c_1\vv_1 + \lambda_2 c_2\vv_2\\
&amp; = P\mqty[\lambda_1c_1 \\ \lambda_2c_2] \\
&amp; = PD\mqty[c_1 \\ c_2] \\
&amp; = PDP^{-1}\bb \text{.}
\end{align*}
</div>
<p class="continuation">Since this equation is true for any <span class="process-math">\(\bb\in\RR^2\text{,}\)</span> it follows that <span class="process-math">\(A = PDP^{-1}\text{.}\)</span></p>
<p id="p-203">The process outlined above and in <a href="" class="xref" data-knowl="./knowl/example-using-an-eigenbasis-to-compute-a-matrix-product.html" title="Example 2.4.1: Using an Eigenbasis to Compute a Matrix Product">Example 2.4.1</a> is known as <em class="emphasis">diagonalization</em>. This is only possible when <span class="process-math">\(A\)</span> has an eigenbasis to work with, but can lead to vastly more efficient computations involving <span class="process-math">\(A\)</span> by making use of the formula</p>
<div class="displaymath process-math">
\begin{equation*}
A^n = PD^nP^{-1}
\end{equation*}
</div>
<p class="continuation">since raising diagonal matrices to a power is much simpler than raising general matrices to a power. Finding <span class="process-math">\(A^{100}\bb\)</span> in <a href="" class="xref" data-knowl="./knowl/example-using-an-eigenbasis-to-compute-a-matrix-product.html" title="Example 2.4.1: Using an Eigenbasis to Compute a Matrix Product">Example 2.4.1</a> was equivalent to the following computations:</p>
<div class="displaymath process-math">
\begin{align*}
A^{100}\bb &amp; = PD^{100}P^{-1}\bb \\
&amp; = \mqty[1 &amp; 3 \\ 1 &amp; 4]\mqty[1^{100} &amp; 0 \\ 0 &amp; 2^{100}]\mqty[4 &amp; -3 \\ -1 &amp; 1]\mqty[-2 \\ 10] \\
&amp; = \mqty[1 &amp; 3 \\ 1 &amp; 4]\mqty[1 &amp; 0 \\ 0 &amp; 2^{100}]\mqty[-38 \\ 12] \\
&amp; = \mqty[1 &amp; 3 \\ 1 &amp; 4]\mqty[-38 \\ 12(2^{100})] \\
&amp; = \mqty[-38 + 36(2^{100}) \\ -38 + 48(2^{100})] \text{.}
\end{align*}
</div>
<article class="definition definition-like" id="definition-diagonalization"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">2.4.3</span><span class="period">.</span><span class="space"> </span><span class="title">Diagonalization.</span>
</h4>
<p id="p-204">A matrix <span class="process-math">\(A\)</span> is <dfn class="terminology">diagonalizable</dfn> if there exists a matrix <span class="process-math">\(P\)</span> and a diagonal matrix <span class="process-math">\(D\)</span> such that</p>
<div class="displaymath process-math">
\begin{equation*}
A = PDP^{-1}\text{.}
\end{equation*}
</div></article><p id="p-205">As mentioned above, a matrix <span class="process-math">\(A\)</span> is diagonalizable if and only if <span class="process-math">\(A\)</span> has an eigenbasis.</p>
<article class="theorem theorem-like" id="theorem-diagonalization-and-eigenbases"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">2.4.4</span><span class="period">.</span><span class="space"> </span><span class="title">Diagonalization and Eigenbases.</span>
</h4>
<p id="p-206">Let <span class="process-math">\(A\)</span> be an <span class="process-math">\(n\times n\)</span> matrix. Then <span class="process-math">\(A\)</span> is diagonalizable if and only if <span class="process-math">\(A\)</span> has a corresponding eigenbasis <span class="process-math">\(\qty{\vv_i}_{i=1}^n\text{.}\)</span></p></article><article class="hiddenproof" id="proof-6"><a href="" data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-6"><h4 class="heading"><span class="type">Proof<span class="period">.</span></span></h4></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-6"><article class="hiddenproof"><p id="p-207">First, assume that <span class="process-math">\(A\)</span> is diagonalizble. Then there exist matrices <span class="process-math">\(P\)</span> and <span class="process-math">\(D\text{,}\)</span> say</p>
<div class="displaymath process-math">
\begin{equation*}
P = \mqty[\vv_1&amp;\ldots&amp;\vv_n]\text{ and }D = \mqty[\lambda_1 &amp; 0 &amp; \ldots &amp; 0 \\ 0 &amp; \lambda_2 &amp; \ldots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \ldots &amp; \lambda_n]
\end{equation*}
</div>
<p class="continuation">such that <span class="process-math">\(P\)</span> is invertible and <span class="process-math">\(A = PDP^{-1}\text{.}\)</span></p>
<p id="p-208">We want to show that <span class="process-math">\(A\)</span> must have an eigenbasis. We'll do this by showing that each column <span class="process-math">\(\vv_i\)</span> of <span class="process-math">\(P\)</span> must be an eigenvector of <span class="process-math">\(A\)</span> with eigenvalue <span class="process-math">\(\lambda_i\text{.}\)</span> Now, since <span class="process-math">\(\vv_i = 0\vv_1 + 0\vv_2 + \cdots + 1\vv_i + \cdots + 0\vv_n\)</span> it follows that <span class="process-math">\(P^{-1}\vv_i\)</span> must be the vector with a single <span class="process-math">\(1\)</span> in the <span class="process-math">\(i^\th\)</span> entry and <span class="process-math">\(0\)</span>s elsewhere. Therefore</p>
<div class="displaymath process-math">
\begin{align*}
A\vv_i &amp; = PDP^{-1}\vv_i \\
&amp; = PD\mqty[0\\0\\\vdots\\1\\\vdots\\0] \\
&amp; = P\mqty[0\\0\\\vdots\\\lambda_i\\\vdots\\0] \\
&amp; = \lambda_i\vv_i 
\end{align*}
</div>
<p class="continuation">and so <span class="process-math">\(\vv_i\)</span> must be an eigenvector of <span class="process-math">\(A\)</span> with eigenvalue <span class="process-math">\(\lambda_i\)</span> for each <span class="process-math">\(i\)</span> from <span class="process-math">\(1\)</span> to <span class="process-math">\(n\text{.}\)</span> Since each column of <span class="process-math">\(P\)</span> is an eigenvector of <span class="process-math">\(A\)</span> and since <span class="process-math">\(P\)</span> is invertible, it follows that the columns must be a basis and, hence, an eigenbasis for <span class="process-math">\(A\text{.}\)</span></p>
<p id="p-209">Now we prove the reverse direction. So assume that <span class="process-math">\(A\)</span> has an eigenbasis <span class="process-math">\(\qty{\vv_i}_{i=1}^n\)</span> with corresponding eigenvalues <span class="process-math">\(\qty{\lambda_i}_{i=1}^n\text{.}\)</span> We will show that <span class="process-math">\(A\)</span> is diagonalized by</p>
<div class="displaymath process-math">
\begin{equation*}
P = \mqty[\vv_1&amp;\ldots&amp;\vv_n]\text{ and }D = \mqty[\lambda_1 &amp; 0 &amp; \ldots &amp; 0 \\ 0 &amp; \lambda_2 &amp; \ldots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \ldots &amp; \lambda_n]\text{.}
\end{equation*}
</div>
<p class="continuation">Let <span class="process-math">\(\xx\in\RR^n\text{.}\)</span> Then we can find the coordinates of <span class="process-math">\(\xx\)</span> with respect to the eigenbasis <span class="process-math">\(\qty{\vv_i}\)</span> by computing <span class="process-math">\(P^{-1}\xx\text{.}\)</span> <aside class="aside aside-like" id="aside-5"><p id="p-210">Note that <span class="process-math">\(P\)</span> is invertible since its columns form a basis!</p></aside> It follows that applying <span class="process-math">\(A\)</span> to <span class="process-math">\(\xx\)</span> is equivalent to applying <span class="process-math">\(PD\)</span> to <span class="process-math">\(P^{-1}\xx\text{:}\)</span> <span class="process-math">\(DP^{-1}\xx\)</span> will multiply the coordinates of <span class="process-math">\(\xx\)</span> with respect to the eigenbasis by the corresponding eigenvalues, and <span class="process-math">\(PDP^{-1}\xx\)</span> reconstructs <span class="process-math">\(A\xx\)</span> using the weights <span class="process-math">\(DP^{-1}\xx\)</span> to form a linear combination of the columns of <span class="process-math">\(P\text{.}\)</span> Therefore <span class="process-math">\(A\xx=PDP^{-1}\xx\)</span> and so <span class="process-math">\(A = PDP^{-1}\text{.}\)</span></p></article></div>
<article class="example example-like" id="example-diagonalizing-a-matrix"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">2.4.5</span><span class="period">.</span><span class="space"> </span><span class="title">Diagonalizing a Matrix.</span>
</h4>
<p id="p-211">Find matrices <span class="process-math">\(P\)</span> and <span class="process-math">\(D\)</span> (if possible) that diagonalize</p>
<div class="displaymath process-math">
\begin{equation*}
A = \mqty[2 &amp; -1 &amp; -1 \\ -1 &amp; 2 &amp; -1 \\ -1 &amp; -1 &amp; 2]\text{.}
\end{equation*}
</div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-22" id="solution-22"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-22"><div class="solution solution-like">
<p id="p-212">By <a href="" class="xref" data-knowl="./knowl/theorem-diagonalization-and-eigenbases.html" title="Theorem 2.4.4: Diagonalization and Eigenbases">Theorem 2.4.4</a>, <span class="process-math">\(A\)</span> is diagonalizble if and only if <span class="process-math">\(A\)</span> has an eigenbasis. Using Octave we quickly see that the eigenvalues of <span class="process-math">\(A\)</span> are <span class="process-math">\(\lambda_1 = 0, \lambda_2 = \lambda_3 = 3\text{.}\)</span> Now we need to <span class="process-math">\(\smqty[A - 0I &amp; \vb{0}]\)</span> and <span class="process-math">\(\smqty[A - 3I &amp; \vb{0}]\text{:}\)</span></p>
<div class="displaymath process-math">
\begin{equation*}
\mqty[A&amp;\vb{0}] \sim \mqty[1 &amp; 0 &amp; -1 &amp; 0 \\ 0 &amp; 1 &amp; -1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0]\text{ and }\mqty[A-3I &amp; \vb{0}] \sim \mqty[1 &amp; 1 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0]\text{.}
\end{equation*}
</div>
<p class="continuation">Therefore</p>
<div class="displaymath process-math">
\begin{equation*}
\nul(A) = \spn{\qty{\mqty[1\\1\\1]}}\text{ and }\nul(A-3I) = \spn{\left\{\mqty[-1\\1\\0],\mqty[-1\\0\\1]\right\}}\text{.}
\end{equation*}
</div>
<p id="p-213">Now we have everything we need to diagonalize <span class="process-math">\(A\text{.}\)</span> Define</p>
<div class="displaymath process-math">
\begin{equation*}
P = \mqty[1 &amp; -1 &amp; -1 \\ 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1]\text{ and }D = \mqty[0 &amp; 0 &amp; 0 \\ 0 &amp; 3 &amp; 0 \\ 0 &amp; 0 &amp; 3]\text{.}
\end{equation*}
</div>
<p class="continuation">Then <span class="process-math">\(A = PDP^{-1}\text{.}\)</span></p>
</div></div>
</div></article><div class="sagecell-octave" id="sage-15"><script type="text/x-sage"># code cell to use for previous example
A = [2, -1, -1; -1, 2, -1; -1, -1, 2]
eig(A)
</script></div></section><section class="subsection" id="subsection-diagonalizations-of-symmetric-and-hermitian-matrices"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber"></span> <span class="title">Diagonalizations of Symmetric and Hermitian Matrices</span>
</h3>
<p id="p-214">Symmetric matrices have particularly nice diagonalizations. First, their eigenvalues must be limited to real numbers.</p>
<article class="theorem theorem-like" id="theorem-eigenvalues-of-symmetric-matrices"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">2.4.6</span><span class="period">.</span><span class="space"> </span><span class="title">Eigenvalues of Symmetric Matrices.</span>
</h4>
<p id="p-215">Let <span class="process-math">\(A\)</span> be a symmetric matrix with real entries and let <span class="process-math">\(\lambda\)</span> be an eigenvalue of <span class="process-math">\(A\text{.}\)</span> Then <span class="process-math">\(\lambda\)</span> must be a real number.</p></article><p id="p-216">The proof of <a href="" class="xref" data-knowl="./knowl/theorem-eigenvalues-of-symmetric-matrices.html" title="Theorem 2.4.6: Eigenvalues of Symmetric Matrices">Theorem 2.4.6</a> is relatively simple but requires us to expand our terminology and notation a bit. First, we redefine the inner product so that it also applies to complex vectors in <span class="process-math">\(\CC^n\text{.}\)</span></p>
<article class="definition definition-like" id="definition-complex-inner-product"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">2.4.7</span><span class="period">.</span><span class="space"> </span><span class="title">Complex Inner Product.</span>
</h4>
<p id="p-217">Let <span class="process-math">\(\xx,\yy\in\CC^n\text{.}\)</span> The <dfn class="terminology">(complex) inner product</dfn> of <span class="process-math">\(\xx\)</span> and <span class="process-math">\(\yy\)</span> is the (complex) scalar</p>
<div class="displaymath process-math">
\begin{equation*}
\langle\xx,\yy\rangle = \yy^{*}\xx
\end{equation*}
</div>
<p class="continuation">where <span class="process-math">\(\yy^*\)</span> denotes the <dfn class="terminology">conjugate transpose</dfn> of <span class="process-math">\(\yy\text{.}\)</span></p></article><p id="p-218">Now we expand our definition of symmetric matrices to include the complex case as well.</p>
<article class="definition definition-like" id="definition-hermitian-matrices"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">2.4.8</span><span class="period">.</span><span class="space"> </span><span class="title">Hermitian Matrices.</span>
</h4>
<p id="p-219">Let <span class="process-math">\(A\)</span> denote a square matrix. Then <span class="process-math">\(A\)</span> is <dfn class="terminology">Hermitian</dfn> if <span class="process-math">\(A = A^{*}\text{.}\)</span></p></article><p id="p-220"><a href="" class="xref" data-knowl="./knowl/definition-hermitian-matrices.html" title="Definition 2.4.8: Hermitian Matrices">Definition 2.4.8</a> generalizes the definition of a real symmetric matrix since <span class="process-math">\(A^* = A^T\)</span> if <span class="process-math">\(A\)</span> only has real entries. The conjugate transpose and inner product also share many properties, including the following.</p>
<article class="proposition theorem-like" id="proposition-conjugate-transpose-and-inner-product"><h4 class="heading">
<span class="type">Proposition</span><span class="space"> </span><span class="codenumber">2.4.9</span><span class="period">.</span><span class="space"> </span><span class="title">Conjugate Transpose and Inner Product.</span>
</h4>
<p id="p-221">Let <span class="process-math">\(\xx,\yy\in\CC^n\)</span> and let <span class="process-math">\(A\)</span> be a square matrix with complex entries. Then</p>
<div class="displaymath process-math">
\begin{equation*}
\langle A\xx,\yy\rangle = \dotprod{\xx,A^*\yy}\text{.}
\end{equation*}
</div></article><p id="p-222">Now we can prove that real symmetric matrices, and more generally Hermitian matrices, always have real eigenvalues.</p>
<article class="theorem theorem-like" id="theorem-eigenvalues-of-a-hermitian-matrix"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">2.4.10</span><span class="period">.</span><span class="space"> </span><span class="title">Eigenvalues of a Hermitian Matrix.</span>
</h4>
<p id="p-223">Let <span class="process-math">\(A\)</span> denote a Hermitian matrix. Then <span class="process-math">\(A\)</span> has real eigenvalues.</p></article><article class="hiddenproof" id="proof-7"><a href="" data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-7"><h4 class="heading"><span class="type">Proof<span class="period">.</span></span></h4></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-7"><article class="hiddenproof"><p id="p-224">Let <span class="process-math">\(\xx\)</span> denote an eigenvector of <span class="process-math">\(A\)</span> with eigenvalue <span class="process-math">\(\lambda\text{.}\)</span> Then</p>
<div class="displaymath process-math">
\begin{align*}
\lambda\dotprod{\xx,\xx} &amp; = \dotprod{\lambda\xx,\xx} \\
&amp; = \dotprod{A\xx,\xx} \\
&amp; = \dotprod{\xx, A^*\xx} \\
&amp; = \dotprod{\xx, A\xx} \\
&amp; = \dotprod{\xx, \lambda \xx} \\
&amp; = \overline{\lambda}\dotprod{\xx, \xx} 
\end{align*}
</div>
<p class="continuation">Since <span class="process-math">\(\dotprod{\xx,\xx} = \norm{\xx}^2\neq0\text{,}\)</span> it follows that <span class="process-math">\(\lambda = \overline{\lambda}\text{.}\)</span> Therefore <span class="process-math">\(\lambda\in\RR\text{.}\)</span></p></article></div>
<p id="p-225">The eigenvectors of a Hermitian matrix also have nice geometric properties.</p>
<article class="theorem theorem-like" id="theorem-eigenvectors-of-a-hermitian-matrix"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">2.4.11</span><span class="period">.</span><span class="space"> </span><span class="title">Eigenvectors of a Hermitian Matrix.</span>
</h4>
<p id="p-226">Let <span class="process-math">\(A\)</span> be a Hermitian matrix. Suppose that <span class="process-math">\(\xx\)</span> and <span class="process-math">\(\yy\)</span> are eigenvectors for two distinct eigenvalues of <span class="process-math">\(A\text{,}\)</span> say <span class="process-math">\(\lambda_i\)</span> and <span class="process-math">\(\lambda_j\text{.}\)</span> Then <span class="process-math">\(\xx\)</span> and <span class="process-math">\(\yy\)</span> are orthogonal.</p></article><article class="hiddenproof" id="proof-8"><a href="" data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-8"><h4 class="heading"><span class="type">Proof<span class="period">.</span></span></h4></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-8"><article class="hiddenproof"><p id="p-227">We need to show that <span class="process-math">\(\dotprod{\xx,\yy} = 0\text{.}\)</span> Now,</p>
<div class="displaymath process-math">
\begin{equation*}
\lambda_i\dotprod{\xx,\yy} = \dotprod{A\xx,\yy} = \dotprod{\xx,A\yy} = \lambda_j\dotprod{\xx,\yy}
\end{equation*}
</div>
<p class="continuation">or just <span class="process-math">\((\lambda_i-\lambda_j)\dotprod{\xx,\yy} = 0\text{.}\)</span> Since <span class="process-math">\(\lambda_i\neq \lambda_j\text{,}\)</span> it follows that <span class="process-math">\(\dotprod{\xx,\yy} = 0\text{.}\)</span></p></article></div>
<p id="p-228"><a href="" class="xref" data-knowl="./knowl/theorem-eigenvectors-of-a-hermitian-matrix.html" title="Theorem 2.4.11: Eigenvectors of a Hermitian Matrix">Theorem 2.4.11</a> leads to an extremely useful fact about real symmetric matrices: they can always be <em class="emphasis">orthogonally diagonalized</em>. This means that we can choose an eigenbasis that is also an orthonormal basis. As an example, consider the eigenbasis found in <a href="" class="xref" data-knowl="./knowl/example-diagonalizing-a-matrix.html" title="Example 2.4.5: Diagonalizing a Matrix">Example 2.4.5</a>, replicated here as columns of the matrix <span class="process-math">\(P\text{:}\)</span></p>
<div class="displaymath process-math">
\begin{equation*}
P = \mqty[1 &amp; -1 &amp; -1 \\ 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1]\text{.}
\end{equation*}
</div>
<p class="continuation">Note that the first column is orthogonal to the other two which is guaranteed by <a href="" class="xref" data-knowl="./knowl/theorem-eigenvectors-of-a-hermitian-matrix.html" title="Theorem 2.4.11: Eigenvectors of a Hermitian Matrix">Theorem 2.4.11</a> since these vectors correspond to different eigenvalues.</p>
<p id="p-229">Although the last two columns are not orthogonal, they can be <em class="emphasis">orthogonalized</em>. One way of doing so is to replace them with the vectors</p>
<div class="displaymath process-math">
\begin{equation*}
\mqty[-1\\1\\0]\text{ and }\mqty[-\frac{1}{2} \\ -\frac{1}{2} \\ 1] = \mqty[-1\\0\\1]-\frac{1}{2}\mqty[-1\\1\\0]\text{.}
\end{equation*}
</div>
<p class="continuation">Both vectors are still eigenvectors with eigenvalue <span class="process-math">\(0\)</span> and the two of them, together with <span class="process-math">\(\smqty[1\\1\\1]\text{,}\)</span> still form an eigenbasis of <span class="process-math">\(\RR^3\text{.}\)</span> However, this new basis is orthogonal.</p>
<p id="p-230">If we go one step further and normalize the vectors in this eigenbasis we then get the orthonormal eigenbasis</p>
<div class="displaymath process-math">
\begin{equation*}
\qty{\mqty[\frac{1}{\sqrt{3}} \\ \frac{1}{\sqrt{3}} \\ \frac{1}{\sqrt{3}}], \mqty[-\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \\ 0], \mqty[-\frac{1}{\sqrt{6}} \\ -\frac{1}{\sqrt{6}} \\ \sqrt{\frac{2}{3}}]}\text{.}
\end{equation*}
</div>
<p class="continuation">This provides the orthogonal diagonalization</p>
<div class="displaymath process-math">
\begin{equation*}
U = \mqty[\frac{1}{\sqrt{3}} &amp; -\frac{1}{\sqrt{2}} &amp; -\frac{1}{\sqrt{6}} \\ \frac{1}{\sqrt{3}} &amp; \frac{1}{\sqrt{2}} &amp; -\frac{1}{\sqrt{6}} \\ \frac{1}{\sqrt{3}} &amp; 0 &amp; \sqrt{\frac{2}{3}}]\text{ and }D = \mqty[0 &amp; 0 &amp; 0 \\ 0 &amp; 3 &amp; 0 \\ 0 &amp; 0 &amp; 3]
\end{equation*}
</div>
<p class="continuation">which gives</p>
<div class="displaymath process-math">
\begin{equation*}
A = UDU^{-1} = UDU^T\text{.}
\end{equation*}
</div>
<p class="continuation">Such a diagonalization is always possible for real symmetric and Hermitian matrices, a result known as the <em class="emphasis">Spectral Theorem</em>.</p>
<article class="theorem theorem-like" id="theorem-spectral-theorem"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">2.4.12</span><span class="period">.</span><span class="space"> </span><span class="title">Spectral Theorem.</span>
</h4>
<p id="p-231">Let <span class="process-math">\(A\)</span> be a (real) symmetric matrix. Then there exists an orthogonal matrix <span class="process-math">\(U\)</span> and a diagonal matrix <span class="process-math">\(D\)</span> such that</p>
<div class="displaymath process-math">
\begin{equation*}
A = UDU^T\text{.}
\end{equation*}
</div>
<p class="continuation">Equivalently, if <span class="process-math">\(\qty{\uu_i}_{i=1}^n\)</span> is an orthonormal eigenbasis of <span class="process-math">\(A\)</span> with corresponding eigenvalues <span class="process-math">\(\qty{\lambda_i}_{i=1}^n\text{,}\)</span> then</p>
<div class="displaymath process-math">
\begin{equation*}
A = \sum_{i=1}^n \lambda_i \uu_i\uu_i^T\text{.}
\end{equation*}
</div></article><article class="example example-like" id="example-a-signal-processing-application"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">2.4.13</span><span class="period">.</span><span class="space"> </span><span class="title">A Signal Processing Application.</span>
</h4>
<p id="p-232">In the mathematics of signal processing, signals are often represented as particular vectors in <span class="process-math">\(\RR^n\text{.}\)</span> The problem of signal transmission then reduces to sending a list of numbers <span class="process-math">\(c_1,\ldots, c_m\)</span> such that the receiver can use these to reconstruct a signal <span class="process-math">\(\xx\text{.}\)</span> Such a scheme can be implemented by choosing an appropriate <span class="process-math">\(n\times m\)</span> matrix <span class="process-math">\(F = \smqty[\vb{f}_1 &amp; \ldots &amp; \vb{f}_m]\)</span> and then computing and transmitting the coordinates of</p>
<div class="displaymath process-math">
\begin{equation*}
F^T\xx = \mqty[\langle \xx,\vb{f}_i\rangle]_{1\leq i\leq m}\text{.}
\end{equation*}
</div>
<p class="continuation">The important properties of the columns of <span class="process-math">\(F\)</span> can then be encoded in the <em class="emphasis">Gram matrix</em> <span class="process-math">\(G = F^TF = \mqty[\dotprod{\vb{f}_i,\vb{f}_j}]\text{.}\)</span> Find a <span class="process-math">\(3\times 4\)</span> matrix <span class="process-math">\(F\)</span> for which that Gram matrix is</p>
<div class="displaymath process-math">
\begin{equation*}
G = \mqty[1 &amp; -\frac{1}{3} &amp; -\frac{1}{3} &amp; -\frac{1}{3} \\ -\frac{1}{3} &amp; 1 &amp; -\frac{1}{3} &amp; -\frac{1}{3} \\ -\frac{1}{3} &amp; -\frac{1}{3} &amp; 1 &amp; -\frac{1}{3} \\ -\frac{1}{3} &amp; -\frac{1}{3} &amp; -\frac{1}{3} &amp; 1]\text{.}
\end{equation*}
</div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-23" id="solution-23"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-23"><div class="solution solution-like"><p id="p-233">This problem can be solved by diagonalizing <span class="process-math">\(G\text{:}\)</span> if <span class="process-math">\(G = UDU^T\)</span> then we can define <span class="process-math">\(F\)</span> by using the rows of <span class="process-math">\(U\sqrt{D}\text{.}\)</span> <aside class="aside aside-like" id="aside-6"><p id="p-234">If <span class="process-math">\(D\)</span> is diagonal with nonnegative entries, then we can define <span class="process-math">\(\sqrt{D}\)</span> to be the matrix obtained by taking the square root of each entry of <span class="process-math">\(D\text{.}\)</span> It's not obvious, but it turns out that Gram matrices always have nonnegative eigenvalues. If we diagonalize <span class="process-math">\(G\)</span> and find it has negative eigenvalues, then it cannot be a Gram matrix.</p></aside> So we'll diagonalize <span class="process-math">\(G\)</span> with the help of the Octave cell below. Doing so, we see that the eigenvalues of <span class="process-math">\(G\)</span> are in fact nonnegative and so <span class="process-math">\(U\sqrt{D}\)</span> will only contain real values. Furthermore, <span class="process-math">\(U\)</span> only has <span class="process-math">\(3\)</span> nonzero columns since <span class="process-math">\(G\)</span> only has three nonzero eigenvalues. By removing the zero column from <span class="process-math">\(U\sqrt{D}\text{,}\)</span> we obtain a <span class="process-math">\(4\times3\)</span> matrix which we define to be <span class="process-math">\(F^T\text{.}\)</span> Then <span class="process-math">\(F^TF = G\text{.}\)</span></p></div></div>
</div></article><div class="sagecell-octave" id="sage-16"><script type="text/x-sage"># diagonalizing a Gram matrix
format short
G = (4/3)*eye(4) - (1/3)*ones(4) # tricky way to define G
</script></div>
<p id="p-235">Symmetric matrices are also useful in analyzing <em class="emphasis">quadratic forms</em>, which are expressions of the form</p>
<div class="displaymath process-math">
\begin{equation*}
\sum_{i,j}c_{ij} x_ix_j
\end{equation*}
</div>
<p class="continuation">where the <span class="process-math">\(x_i\)</span> are variables and the <span class="process-math">\(c_{ij}\)</span> are the coefficients. Such an expression can be rewritten as <span class="process-math">\(\xx^T A\xx\)</span> where <span class="process-math">\(A\)</span> is a symmetric matrix determined from the coefficients.</p>
<article class="example example-like" id="example-analyzing-a-quadratic-form"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">2.4.14</span><span class="period">.</span><span class="space"> </span><span class="title">Analyzing a Quadratic Form.</span>
</h4>
<p id="p-236">Describe the curve given by</p>
<div class="displaymath process-math">
\begin{equation*}
9x^2 + 6xy + y^2 = 10\text{.}
\end{equation*}
</div>
<div class="solutions">
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-24" id="solution-24"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-24"><div class="solution solution-like">
<p id="p-237">The left hand side is a quadratic form with variables <span class="process-math">\(x_1 = x,x_2 = y\)</span> and coefficients</p>
<div class="displaymath process-math">
\begin{equation*}
c_{11} = 9, c_{12} = c_{21} = \frac{6}{2}=3\text{ and }c_{22} = 1\text{.}
\end{equation*}
</div>
<p class="continuation">Therefore we can write <span class="process-math">\(9x^2 + 6xy + y^2 = \xx^T A\xx\)</span> where</p>
<div class="displaymath process-math">
\begin{equation*}
\xx = \mqty[x\\y]\text{ and }A = \mqty[9 &amp; 3 \\ 3 &amp; 1]\text{.}
\end{equation*}
</div>
<p class="continuation">To help us describe the curve we will “disentangle” the variables <span class="process-math">\(x\)</span> and <span class="process-math">\(y\)</span> by diagonalizing <span class="process-math">\(A\text{.}\)</span></p>
<p id="p-238">Since <span class="process-math">\(A\)</span> is symmetric, we know that <span class="process-math">\(A\)</span> can be orthogonally diagonalized. One such diagonalization is given by <span class="process-math">\(A = UDU^T\)</span> where</p>
<div class="displaymath process-math">
\begin{equation*}
U = \mqty[\frac{1}{\sqrt{10}} &amp; \frac{3}{\sqrt{10}} \\ -\frac{3}{\sqrt{10}} &amp; \frac{1}{\sqrt{10}}]\text{ and } D = \mqty[0 &amp; 0 \\ 0 &amp; 10]\text{.}
\end{equation*}
</div>
<p class="continuation">The quadratic form <span class="process-math">\(\xx^T A\xx\)</span> then becomes</p>
<div class="displaymath process-math">
\begin{equation*}
\xx^T UDU^T\xx = (U^T\xx)^T D (U^T \xx)\text{.}
\end{equation*}
</div>
<p id="p-239">Now we define <span class="process-math">\(\yy = U^T\xx\)</span> as a change-of-variables. In particular,</p>
<div class="displaymath process-math">
\begin{equation*}
\yy = \mqty[X\\Y] = \mqty[\frac{1}{\sqrt{10}}x - \frac{3}{\sqrt{10}}y \\ \frac{3}{\sqrt{10}}x + \frac{1}{\sqrt{10}}y]\text{.}
\end{equation*}
</div>
<p class="continuation">Our quadratic form is now</p>
<div class="displaymath process-math">
\begin{equation*}
\yy^T D\yy = 0X^2 + 10Y^2 = 10Y^2
\end{equation*}
</div>
<p class="continuation">and our original equation becomes <span class="process-math">\(10Y^2 = 10\)</span> or just <span class="process-math">\(Y = \pm1\text{.}\)</span> Therefore the original equation describes the two different lines</p>
<div class="displaymath process-math">
\begin{equation*}
\frac{3}{\sqrt{10}}x + \frac{1}{\sqrt{10}}y = -1\text{ and }\frac{3}{\sqrt{10}}x + \frac{1}{\sqrt{10}}y = 1\text{.}
\end{equation*}
</div>
</div></div>
</div></article></section><section class="subsection" id="subsection-analytic-functions-of-matrices"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber"></span> <span class="title">Analytic Functions of Matrices</span>
</h3>
<p id="p-240">A function <span class="process-math">\(f(x)\)</span> is <em class="emphasis">analytic</em> at <span class="process-math">\(x = a\)</span> if it has a power series representation on some interval centered around <span class="process-math">\(a\text{:}\)</span></p>
<div class="displaymath process-math">
\begin{equation*}
f(x) = \sum_{k=0}^\infty c_k(x-a)^k = c_0 + c_1(x-a) + c_2(x-a)^2 + \cdots\text{ for }x\approx a\text{.}
\end{equation*}
</div>
<p class="continuation">If <span class="process-math">\(A\)</span> denotes a square matrix, and if <span class="process-math">\(f(x)\)</span> is analytic at <span class="process-math">\(a=0\text{,}\)</span> then we can try to make sense of the expression <span class="process-math">\(f(A)\)</span> by using the power series for <span class="process-math">\(f(x)\text{:}\)</span></p>
<div class="displaymath process-math">
\begin{equation*}
f(A) = \sum_{k=0}^{\infty}c_k A^k = c_0I + c_1A + c_2A^2 + \cdots\text{,}
\end{equation*}
</div>
<p class="continuation">assuming this sum actually exists.</p>
<article class="example example-like" id="example-exponential-of-a-matrix"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">2.4.15</span><span class="period">.</span><span class="space"> </span><span class="title">Exponential of a Matrix.</span>
</h4>
<p id="p-241">Let</p>
<div class="displaymath process-math">
\begin{equation*}
A = \mqty[0 &amp; 2 &amp; 3 \\ 0 &amp; 0 &amp; 2 \\ 0 &amp; 0 &amp; 0]\text{.}
\end{equation*}
</div>
<p class="continuation">Find <span class="process-math">\(e^A\text{.}\)</span></p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-25" id="solution-25"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-25"><div class="solution solution-like">
<p id="p-242">By definition,</p>
<div class="displaymath process-math">
\begin{equation*}
e^A = I + A + \frac{1}{2!}A^2 + \frac{1}{3!}A^3 + \cdots\text{,}
\end{equation*}
</div>
<p class="continuation">so we can find <span class="process-math">\(e^A\)</span> by looking at the powers of <span class="process-math">\(A\text{.}\)</span> In this case,</p>
<div class="displaymath process-math">
\begin{equation*}
A^2 = \mqty[0 &amp; 0 &amp; 4 \\ 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0]\text{ and }A^3 = A^4 = \ldots = \mqty[0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0]\text{.}
\end{equation*}
</div>
<p class="continuation">Therefore</p>
<div class="displaymath process-math">
\begin{equation*}
e^A = I + A + \frac{1}{2}A^2 = \mqty[1 &amp; 2 &amp; 5 \\ 0 &amp; 1 &amp; 2 \\ 0 &amp; 0 &amp; 1]\text{.}
\end{equation*}
</div>
<p class="continuation">This can be verified using the command <code class="code-inline tex2jax_ignore">expm</code> in Octave as below. <aside class="aside aside-like" id="aside-7"><p id="p-243">You want to be careful to use <code class="code-inline tex2jax_ignore">expm(A)</code> to compute <span class="process-math">\(e^A\)</span> in Octave, as this actually finds the matrix exponential. If you instead use <code class="code-inline tex2jax_ignore">exp(A)</code>, this just computes the matrix obtained by raising <span class="process-math">\(e\)</span> to each entry of <span class="process-math">\(A\text{,}\)</span> which in general is <em class="emphasis">not</em> equal to <span class="process-math">\(e^A\text{.}\)</span></p></aside></p>
</div></div>
</div></article><div class="sagecell-octave" id="sage-17"><script type="text/x-sage">format short
A = [0, 2, 3; 0, 0, 2; 0, 0, 0]
expm(A) # don't use exp(A)! that just exponentiates each entry of A
</script></div>
<article class="example example-like" id="example-function-of-a-diagonal-matrix"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">2.4.16</span><span class="period">.</span><span class="space"> </span><span class="title">Function of a Diagonal Matrix.</span>
</h4>
<p id="p-244">Let <span class="process-math">\(f(x)\)</span> be a function analytic at <span class="process-math">\(0\text{.}\)</span> Let <span class="process-math">\(D\)</span> be a diagonal matrix whose diagonal entries are within the interval of convergence of the power series for <span class="process-math">\(f(x)\)</span> centered at <span class="process-math">\(0\text{.}\)</span> Find <span class="process-math">\(f(D)\text{.}\)</span></p></article><p id="p-245">The last example shows that if <span class="process-math">\(f(x)\)</span> is analytic at <span class="process-math">\(0\)</span> then it is relatively straightforward to find <span class="process-math">\(f(D)\text{,}\)</span> assuming that the diagonal entries of <span class="process-math">\(D\)</span> are within the interval of convergence for the series representation of <span class="process-math">\(f(x)\)</span> at <span class="process-math">\(0\text{.}\)</span> Therefore we can find functions of diagonalizable matrices as well.</p>
<article class="theorem theorem-like" id="theorem-analytic-functions-of-diagonalizable-matrices"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">2.4.17</span><span class="period">.</span><span class="space"> </span><span class="title">Analytic Functions of Diagonalizable Matrices.</span>
</h4>
<p id="p-246">Let <span class="process-math">\(f(x)\)</span> be a function that is analytic at <span class="process-math">\(0\)</span> and whose series representation at <span class="process-math">\(0\)</span> has interval of convergence <span class="process-math">\(I\text{.}\)</span> Let <span class="process-math">\(A\)</span> be a diagonalizable matrix diagonalized by <span class="process-math">\(P\)</span> and <span class="process-math">\(D\)</span> with eigenvalues contained in <span class="process-math">\(I\text{.}\)</span> Then</p>
<div class="displaymath process-math">
\begin{equation*}
f(A) = Pf(D)P^{-1}\text{.}
\end{equation*}
</div></article><article class="hiddenproof" id="proof-9"><a href="" data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-9"><h4 class="heading"><span class="type">Proof<span class="period">.</span></span></h4></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-9"><article class="hiddenproof"><p id="p-247">Let <span class="process-math">\(f(x) = \sum_{k=0}^{\infty}c_k x^k\)</span> and recall that <span class="process-math">\(A^k = PD^kP^{-1}\text{.}\)</span> Then</p>
<div class="displaymath process-math">
\begin{equation*}
f(A) = \sum_{k=0}^{\infty}c_kA^k = P\qty(\sum_{k=0}^{\infty}c_k D^k)P^{-1} = Pf(D)P^{-1}\text{.}
\end{equation*}
</div></article></div>
<p id="p-248"><a href="" class="xref" data-knowl="./knowl/theorem-analytic-functions-of-diagonalizable-matrices.html" title="Theorem 2.4.17: Analytic Functions of Diagonalizable Matrices">Theorem 2.4.17</a> allows for straightforward computations of matrix exponentials of diagonalizable matrices. This is useful in differential equations when solving linear systems of ODEs (see <a class="external" href="https://j-oldroyd.github.io/wvwc-differential-equations/systems-of-odes.html" target="_blank">here</a><a href="" data-knowl="" class="id-ref fn-knowl original" data-refid="hk-fn-3" id="fn-3"><sup> 3 </sup></a>).</p>
<article class="theorem theorem-like" id="theorem-exponential-solutions-of-linear-systems-of-odes"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">2.4.18</span><span class="period">.</span><span class="space"> </span><span class="title">Exponential Solutions of Linear Systems of ODEs.</span>
</h4>
<p id="p-249">Let <span class="process-math">\(A\)</span> be a (constant) square matrix and consider the first-order system <span class="process-math">\(\xx^\prime = A\xx\)</span> with initial condition <span class="process-math">\(\xx(0) = \xx_0\text{.}\)</span> Then the solution of this initial value problem is</p>
<div class="displaymath process-math">
\begin{equation*}
\xx(t) = e^{At}\xx_0\text{.}
\end{equation*}
</div></article><article class="hiddenproof" id="proof-10"><a href="" data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-10"><h4 class="heading"><span class="type">Proof<span class="period">.</span></span></h4></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-10"><article class="hiddenproof"><p id="p-250">The proof follows quickly from the fact that <span class="process-math">\(\dv{}{t}(e^{At}) = Ae^{At}\text{.}\)</span> Using this, we differentiate <span class="process-math">\(\xx(t) = e^{At}\xx_0\)</span> to get</p>
<div class="displaymath process-math">
\begin{equation*}
\dv{\xx}{t} = Ae^{At}\xx_0 = A\xx\text{.}
\end{equation*}
</div>
<p class="continuation">Furthermore, <span class="process-math">\(\xx(0) = e^{\vb{0}}\xx_0 = \xx_0\text{.}\)</span> Therefore <span class="process-math">\(\xx(t) = e^{At}\xx_0\)</span> is a solution of the initial value problem.</p></article></div>
<p id="p-251">Theorems <a href="" class="xref" data-knowl="./knowl/theorem-analytic-functions-of-diagonalizable-matrices.html" title="Theorem 2.4.17: Analytic Functions of Diagonalizable Matrices">2.4.17</a> and <a href="" class="xref" data-knowl="./knowl/theorem-exponential-solutions-of-linear-systems-of-odes.html" title="Theorem 2.4.18: Exponential Solutions of Linear Systems of ODEs">2.4.18</a> allow us to find solutions of linear systems that involve diagonalizable matrices in terms of the matrix exponential.</p>
<article class="example example-like" id="example-solving-a-first-order-system"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">2.4.19</span><span class="period">.</span><span class="space"> </span><span class="title">Solving a First-Order System.</span>
</h4>
<p id="p-252">Solve</p>
<div class="displaymath process-math">
\begin{equation*}
x^\prime = 3x - 4y \text{ and } y^\prime = -4x + 3y
\end{equation*}
</div>
<p class="continuation">with initial condition <span class="process-math">\(x(0) = 2\)</span> and <span class="process-math">\(y(0) = -1\text{.}\)</span></p>
<div class="solutions">
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-26" id="solution-26"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-26"><div class="solution solution-like">
<p id="p-253">This can be solved very easily using the matrix exponential and diagonalization. If we let</p>
<div class="displaymath process-math">
\begin{equation*}
\xx = \mqty[x \\ y]\text{ and }A = \mqty[3 &amp; -4 \\ -4 &amp; 3]
\end{equation*}
</div>
<p class="continuation">then the system can be written as the matrix ODE <span class="process-math">\(\xx^\prime = A\xx\)</span> with initial condition <span class="process-math">\(\xx_0 = \smqty[2 \\ 1]\text{.}\)</span> <span class="process-math">\(A\)</span> is symmetric and can be orthogonally diagonalized by</p>
<div class="displaymath process-math">
\begin{equation*}
U = \mqty[\frac{\sqrt{2}}{2} &amp; \frac{\sqrt{2}}{2} \\ \frac{\sqrt{2}}{2} &amp; -\frac{\sqrt{2}}{2}]\text{ and } D = \mqty[-1 &amp; 0 \\ 0 &amp; 7]
\end{equation*}
</div>
<p class="continuation">as seen in the code cell below this example. Therefore <span class="process-math">\(A = UDU^T\text{,}\)</span> <span class="process-math">\(e^{At} = Ue^{Dt}U^T\)</span> and the solution of the system must be</p>
<div class="displaymath process-math">
\begin{equation*}
\xx = \mqty[\frac{\sqrt{2}}{2} &amp; \frac{\sqrt{2}}{2} \\ \frac{\sqrt{2}}{2} &amp; -\frac{\sqrt{2}}{2}]\mqty[e^{-t} &amp; 0 \\ 0 &amp; e^{7t}]\mqty[\frac{\sqrt{2}}{2} &amp; \frac{\sqrt{2}}{2} \\ \frac{\sqrt{2}}{2} &amp; -\frac{\sqrt{2}}{2}]\mqty[2 \\ -1]\text{.}
\end{equation*}
</div>
</div></div>
</div></article><div class="sagecell-octave" id="sage-18"><script type="text/x-sage">format short
A = [3, -4; -4, 3]
[U,D] = eig(A)
</script></div></section></section><div class="hidden-content tex2jax_ignore" id="hk-fn-3"><div class="fn"><code class="code-inline tex2jax_ignore">j-oldroyd.github.io/wvwc-differential-equations/systems-of-odes.html</code></div></div>
</div></main>
</div>
</body>
</html>
