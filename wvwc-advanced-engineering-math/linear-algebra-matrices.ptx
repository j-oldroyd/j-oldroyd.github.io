<?xml version="1.0" encoding="UTF-8" ?>

<!--********************************************************************

*********************************************************************-->
<!-- This file was originally part of the book     -->
<!-- (as copied on 2015/07/12)                     -->
<!--                                               -->
<!--   Abstract Algebra: Theory and Applications   -->
<!--                                               -->
<!-- Copyright (C) 1997-2014  Thomas W. Judson     -->

<chapter xml:id="chapter-linear-algebra-matrices" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Introduction to Matrix Algebra</title>
  <section xml:id="section-matrices-vectors-and-linear-combinations">
    <title>Matrices, Vectors and Linear Combinations</title>
    <p>
      The primary objects of study in the field of linear algebra and its applications are linear transformations between vector spaces.
      These linear transformations are often represented using <em>matrices</em>.
    </p>
    <definition xml:id="definition-matrix">
      <idx><h>matrices</h><h>definition</h></idx>
      <title>Matrix</title>
      <statement>
        <p>
          A <term>matrix</term> is a rectangular array of numbers.
          If this array has <m>m</m> rows and <m>n</m> columns, we say the matrix is an <m>m\times n</m> matrix.
        </p>
      </statement>
    </definition>
    <p>
      The following are examples of matrices:
      <me>
        \begin{bmatrix} 1 \amp 2 \\ -3 \amp 4 \end{bmatrix}\text{ and }\begin{bmatrix}1 \amp -2 \amp 0 \\ 3 \amp -21 \amp 2\end{bmatrix}
      </me>
      The first is <m>2\times2</m> and the second is <m>2\times3</m>.
    </p>
    <p>
      We say that a matrix is a <term>square matrix</term> if it has the same number of rows as columns.
      <me>A = [a_{ij}] = \begin{bmatrix}
        a_{11} \amp a_{12} \amp \dots \amp a_{1n} \\
        a_{21} \amp a_{22} \amp \dots \amp a_{2n} \\
        \vdots \amp \vdots \amp \ddots \amp \vdots \\
        a_{n1} \amp a_{n2} \amp \dots \amp a_{nn}
        \end{bmatrix},
      </me>
      The <term>diagonal entries</term> are <m>a_{ii},1\leq i\leq n</m> and these form the <term>main diagonal</term> of the matrix.
    </p>
    <p>
      As important as matrices are in applications of mathematics, many computing solutions exist for handling computations involving them.
      One open source solution (included in Sage/CoCalc!) is <url href="https://www.gnu.org/software/octave/index" visual="https://www.gnu.org/software/octave/index">Octave</url>, which is a free alternative to MATLAB.
      In the code cell below Octave is used to define the square matrix above and get its diagonal entries.
      Note that brackets must be used to contain the entries of the matrix, entries in the same row must be separated by commas (or spaces) and rows are separated by semicolons.
    </p>
    <sage language="octave">
      <input>
        A = [1, 2; -3, 4]
        diag(A)
      </input>
    </sage>
  </section>
  <section xml:id="section-matrix-multiplication">
    <title>Matrix Multiplication</title>
    <p>
      To be completed.
    </p>
  </section>
  <section xml:id="section-systems-of-linear-equations">
    <title>Systems of Linear Equations</title>
    <p>
      To be completed.
    </p>
  </section>
  <section xml:id="section-linear-independence">
    <title>Linear Independence</title>
    <p>
      To be completed.
    </p>
    <definition xml:id="definition-pivot-columns">
      <title>Pivot Columns</title>
      <idx><h>echelon form</h><h>pivot columns</h></idx>
      <statement>
        <p>
          Let <m>A</m> be a matrix.
          The <term>pivot columns</term> of <m>A</m> are those columns which contain leading entries in any echelon form of <m>A</m>.
        </p>
      </statement>
    </definition>
    <theorem xml:id="theorem-rank-and-pivot-columns">
      <title>Rank and Pivot Columns</title>
      <statement>
        <p>
          Let <m>A</m> be a matrix.
          Then <m>\rank{A}</m> is exactly equal to the number of pivot columns of <m>A</m>.
        </p>
      </statement>
    </theorem>
    <definition xml:id="definition-column-space">
        <title>Column Space</title>
        <idx>column space</idx>
        <statement>
          <p>
            The <term>column space</term> of a matrix <m>A</m> is the span of the columns of <m>A</m>.
            Equivalently, the column space is the set of all vectors of the form <m>A\vb{x}</m>.
            The column space of <m>A</m> is denoted by <m>\col{A}</m>.
          </p>
        </statement>
    </definition>
  </section>
  <section xml:id="section-existence-of-solutions">
    <title>Existence of Solutions</title>
    <p>
      Recall that any linear system may be expressed as a matrix equation of the form <m>A\vb{x} = \vb{b}</m>.
      Based on our previous work, we can make the following observations.
    </p>
    <theorem xml:id="theorem-consistency-rank-and-the-column-space">
      <title>Consistency, Rank and the Column Space</title>
      <idx><h>rank</h><h>consistency of systems</h></idx><idx><h>rank</h><h>consistency of systems</h><seealso>column space</seealso></idx>
      <statement>
        <p>
          Let <m>A</m> be an <m>m\times n</m> matrix, <m>\vb{x}\in\RR^{n}</m> and <m>\vb{b}\in\RR^{m}</m>.
          The linear system <m>A\vb{x} = \vb{b}</m> is consistent if and only if <m>\vb{b}\in\col{A}</m>.
          Equivalently, the system is consistent if and only if <m>\rank{A} = \rank{\mqty[A \amp \vb{b}]}</m>.
          Furthermore, the system has precisely one solution if <m>\rank{A} = \rank{\mqty[A \amp \vb{b}]} = n</m> and has infinitely many solutions if <m>\rank{A} = \rank{\mqty[A \amp \vb{b}]} \lt n</m>.
        </p>
      </statement>
    </theorem>
    <p>
      As before, we use Gaussian elimination (i.e., row reduction) to solve systems.
    </p>
    <example xml:id="example-free-variable-solution-vector-form">
      <statement>
        <p>
          Find any solutions of
          <md>
            <mrow>5x - 7y + 3z \amp= 17</mrow>
            <mrow>-15x + 20y - 9z \amp= -50 </mrow>
          </md>
        </p>
      </statement>
      <solution>
        <p>
          Reducing to an echelon form is enough to determine if the system is consistent and find the number of solutions it has.
          Using Octave to find the reduced echelon form (see the code cell immediately after this example), we get
          <me>\begin{bmatrix} 1 \amp 0 \amp \frac{3}{5} \amp 2 \\ 0 \amp 1 \amp 0 \amp -1\end{bmatrix}.</me>
        </p>
        <p>
          From the reduced echelon form above, we see that the system must be consistent since the rank of the coefficient matrix is equal to the rank of the augmented matrix.
          Equivalently, the last column is not a pivot column.
          Furthermore, there are infinitely many solutions since the rank of the coefficient matrix is less than the total number of columns.
        </p>
        <p>
          The solution set itself can be written in vector notation as
          <me>\begin{bmatrix}x \\ y \\ z\end{bmatrix} = \begin{bmatrix}2 \\ -1 \\ 0\end{bmatrix} + z\begin{bmatrix}-\frac{3}{5} \\ 0 \\ 1\end{bmatrix}.</me>
          This, again, is verified below.
        </p>
      </solution>
    </example>
    <sage language="octave">
      <input>
        A = [5, -7, 3; -15, 20, -9];
        b = [17; -50];
        rref([A, b]) % reduced echelon form of augmented matrix

        % verify solution
        z = 3.1; % arbitrary value for z
        x_soln = [2; -1; 0] + z*[-3/5; 0; 1]; % solution vector
        A*x_soln % equals b
      </input>
    </sage>
    <p>
      In the last example the variable <m>z</m> led to infinitely many solutions and we were able to write out solution depending on the value of this variable.
      We call <m>z</m> a <em>free variable</em> and <m>x</m> and <m>y</m> <em>basic variables</em>.
    </p>
    <definition xml:id="definition-basic-and-free-variables">
      <title>Basic and Free Variables</title>
      <idx><h>linear systems</h><h>basic and free variables</h></idx><idx><h>linear systems</h><h>basic and free variables</h><seealso>pivot columns</seealso></idx>
      <statement>
        <p>
          Given a consistent linear system <m>A\vb{x}=\vb{b}</m>, the variables corresponding to pivot columns of <m>A</m> are <term>basic variables</term> and the variables corresponding to non-pivot columns of <m>A</m> are <term>free variables</term>.
        </p>
      </statement>
    </definition>
    <p>
      Any solution of the linear system <m>A\vb{x} = \vb{b}</m> can always be written to depend solely on any free variables as we did in <xref ref="example-free-variable-solution-vector-form" text="type-global" />.
      In fact we can go a bit further, still using our answer in <xref ref="example-free-variable-solution-vector-form" text="type-global" /> as a guide.
      Using free variables, any solution to <m>A\vb{x}=\vb{b}</m> can be written as a sum of two components:
      <me>\vb{x} = \vb{x}_p + \vb{x}_{\text{free}}.</me>
      This notation will change shortly, but the main idea is that one component of the solution will not depend on the free variable and will represent a single solution of the system <m>A\vb{x} = \vb{b}</m>.
      In <xref ref="example-free-variable-solution-vector-form" text="type-global" /> this would be
      <me>\vb{x}_{p} = \mqty[2\\-1\\0],</me>
      and it's easy to verify that
      <me>A\vb{x}_{p} = \mqty[5 \amp -7 \amp 3 \\ -15 \amp 20 \amp -9]\mqty[2\\-1\\0] = \mqty[17\\-50].</me>
      The <em>other</em> component of the solution, <m>\vb{x}_{\text{free}}</m>, will depend on the free variable.
      In <xref ref="example-free-variable-solution-vector-form" text="type-global" />, this component was
      <me>\vb{x}_{\text{free}} = z\mqty[-\frac{3}{5} \\ 0 \\ 1].</me>
      As it turns out, this component is <em>not</em> a solution of the original system <m>A\vb{x} = \vb{b}</m>.
      Instead, <m>A\vb{x}_{\text{free}} = \vb{0}</m>.
      This behavior is shared by all consistent systems with free variables, and leads us to introduce the following terminology.
    </p>
    <definition xml:id="definition-associated-homogeneous-system">
      <title>Associated Homogeneous System</title>
      <idx><h>linear systems</h><h>associated homogeneous system</h></idx>
      <statement>
        <p>
          Given a linear system <m>A\vb{x} = \vb{b}</m>, we define the <term>associated homogeneous system</term> to be the system <m>A\vb{x} = \vb{0}</m>.
        </p>
      </statement>
    </definition>
    <p>
      The observations made after <xref ref="example-free-variable-solution-vector-form" text="type-global" /> can be summarized in the following theorem.
    </p>
    <theorem xml:id="theorem-particular-and-homogeneous-solutions">
      <title>Particular and Homogeneous Solutions</title>
      <statement>
        <p>
          Suppose that <m>A\vb{x} = \vb{b}</m> is a consistent linear system.
          Then the general solution <m>\vb{x}</m> can be written in the form <m>\vb{x} = \vb{x}_{p} + \vb{x}_{h}</m> where <m>\vb{x}_{p}</m> is a single solution of the original system <m>A\vb{x} = \vb{b}</m> and <m>\vb{x}_{h}</m> is the general solution of the associated homogeneous system <m>A\vb{x} = \vb{0}</m>.
          We call <m>\vb{x}_{p}</m> a <term>particular solution</term>.
        </p>
      </statement>
      <proof>
        <p>
          Here, we only prove that <m>\vb{x}_{h}</m> satisfies the associated homogeneous system.
          Since <m>\vb{x}_{p}</m> is a solution of the original system along with <m>\vb{x} = \vb{x}_{p} + \vb{x}_{h}</m>, it follows that
          <me>A\vb{x}_{h} = A(\vb{x}-x_{p}) = \vb{b} - \vb{b} = \vb{0}.</me>
          Therefore <m>\vb{x}_{h}</m> is a solution of the associated homogeneous system.
        </p>
      </proof>
    </theorem>
    <p>
      Since solutions of homogeneous systems play such an important role in the solution of non-homogeneous systems, we give their solution sets a special name.
    </p>
    <definition xml:id="definition-null-space">
      <title>Null Space</title>
      <idx>null space</idx>
      <statement>
        <p>
          Let <m>A</m> be a matrix.
          The <term>null space</term> of <m>A</m> is the set of all solutions of <m>A\vb{x} = \vb{0}</m>.
          This is denoted by <m>\null{A}</m>.
        </p>
      </statement>
    </definition>
    <p>
      As with column spaces and row spaces, null spaces are always subspaces.
    </p>
    <theorem xml:id="theorem-the-null-space-is-a-subspace">
      <title>The Null Space is a Subspace</title>
      <statement>
        <p>
          Let <m>A</m> be an <m>m\times n</m> matrix.
          Then <m>\null{A}</m> is a subspace of <m>\RR^n</m>.
        </p>
      </statement>
      <proof>
        <p>
          To show that <m>\null{A}</m> is a subspace we need to show that it's closed under linear combinations.
          So let <m>\vb{u},\vb{v}\in\null{A}</m> be arbitrary vectors in the null space and let <m>\alpha,\beta\in\RR</m> be arbitrary scalars.
          Our goal is to show that <m>\alpha\vb{u} + \beta\vb{v}\in\null{A}</m>.
          Thankfully, we can do this very quickly:
          <md>
            <mrow> A(\alpha\vb{u} + \beta\vb{v}) \amp= \alpha A\vb{u} + \beta A\vb{v}</mrow>
            <mrow> \amp= \vb{0} + \vb{0} </mrow>
            <mrow> \amp= \vb{0}, </mrow>
          </md>
          which shows that the linear combination <m>\alpha\vb{u}+\beta\vb{v}</m> lies in <m>\null{A}</m>.
        </p>
      </proof>
    </theorem>
    <p>
      The concept of the null space is related to that of the column space in <xref ref="definition-column-space" text="type-global" />, but they are distinct.
      To be precise, if <m>A</m> is an <m>m\times n</m> matrix then
      <md>
        <mrow>\col{A} \amp= \qty{A\vb{x} : \vb{x}\in\RR^n} \subseteq \RR^{m} </mrow>
        <mrow>\null{A} \amp= \qty{\vb{x} : A\vb{x} = \vb{0}} \subseteq \RR^{n} </mrow>
      </md>
    </p>
    <example xml:id="example-finding-a-null-space">
      <title>Finding a Null Space</title>
      <statement>
        <p>
          Let
          <me>A = \mqty[0 \amp 5 \amp 5 \amp -10 \amp 0 \\ 2 \amp -3 \amp -3 \amp 6 \amp 2 \\ 4 \amp 1 \amp 1 \amp -2 \amp 4].</me>
          Find <m>\null{A}</m>.
        </p>
      </statement>
      <solution>
        <p>
          We need to find the solution set of <m>A\vb{x} = \vb{0}</m> which we've done before.
          The Octave code cell below can be used to solve this system, giving the reduced echelon form for the augmented matrix <m>\mqty[A \amp \vb{0}]</m> to be
          <me>\mqty[1 \amp 0 \amp 0 \amp 0 \amp 1 \amp 0 \\ 0 \amp 1 \amp 1 \amp -2 \amp 0 \amp 0 \\ 0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0].</me>
          Therefore any solution <m>\vb{x}\in\RR^5</m> of <m>A\vb{x}=\vb{0}</m> must look like
          <me>\vb{x} = \mqty[x_1\\x_2\\x_3\\x_4\\x_5] = \mqty[-x_5\\-x_3+2x_4\\x_3\\x_4\\x_5] = x_3\mqty[0\\-1\\1\\0\\0] + x_4\mqty[0\\2\\0\\1\\0] + x_5\mqty[-1\\0\\0\\0\\1].</me>
        </p>
        <p>
          Note that the above shows that
          <me>\null{A} = \span{\qty{\mqty[0\\-1\\1\\0\\0], \mqty[0\\2\\0\\1\\0], \mqty[-1\\0\\0\\0\\1]}}</me>
          Since these vectors are also linearly independent, it follows that the set
          <me>\qty{\mqty[0\\-1\\1\\0\\0], \mqty[0\\2\\0\\1\\0], \mqty[-1\\0\\0\\0\\1]}</me>
          is in fact a basis for <m>\null{A}</m> and <m>\dim{\null{A}} = 3</m>.
        </p>
      </solution>
    </example>
    <sage language="octave">
      <input>
        % code cell for finding null space in previous example
      </input>
    </sage>
    <p>
      At this point we can make a simple but useful observation.
      In <xref ref="example-finding-a-null-space" text="type-global" />, the dimension of the null space was directly tied to the number of free variables in the system <m>A\vb{x} = \vb{0}</m>.
      The number of basic variables is likewise equal to the number of pivot columns of <m>A</m>.
      Noting that the number of basic variables plus the number of free variables must be the total number of columns of <m>A</m>, together with <xref ref="theorem-rank-and-pivot-columns" text="type-global" />, we get the <em>Rank-Nullity Theorem</em>.
    </p>
    <theorem xml:id="theorem-rank-nullity-theorem">
      <title>Rank-Nullity Theorem</title>
      <idx>Rank-Nullity Theorem</idx>
      <statement>
        <p>
          Let <m>A</m> be an <m>m\times n</m> matrix.
          Then
          <me>\rank{A} + \dim\null{A} = n.</me>
        </p>
      </statement>
    </theorem>
  </section>
  <section xml:id="section-determinants">
    <title>Determinants</title>
    <p>
      If <m>A</m> is an <m>n\times n</m> square matrix and if <m>\vb{x}\in\RR^n</m>, then <m>\vb{x}</m> and <m>A\vb{x}</m> have the same size.
      In other words, both <m>\vb{x}</m> and <m>A\vb{x}</m> live in the same vector space <m>\RR^n</m>.
      This makes some geometry involving <m>A</m> slightly easier.
      In particular, <m>\col{A}</m> must be a subspace of <m>\RR^n</m>.
    </p>
    <p>
      If <m>\col{A}</m> is a subspace of <m>\RR^n</m> then we can say that the linear system <m>A\vb{x} = \vb{b}</m> is always consistent if and only if <m>\col{A} = \RR^n</m>.
      If this were not the case, then there would exist some vector <m>\vb{b}\in\RR^n</m> such that <m>\vb{b}\notin\col{A}</m> and so <m>A\vb{x}=\vb{b}</m> would have to be inconsistent.
      So square matrices <m>A</m> for which <m>\col{A}=\RR^n</m> are particularly well-behaved and useful.
      Therefore we'd like to develop conditions to check for when a square matrix <m>A</m> satisfied this property.
    </p>
    <p>
      One possible condition for this is the following: <m>\col{A}=\RR^n</m> if and only if <m>\dim\col{A}=n</m>, which happens if and only if <m>\rank{A}=n</m>.
      Therefore <m>A\vb{x}=\vb{b}</m> is always consistent if and only if <m>\rank{A}=n</m>.
      However, another useful condition which uses geometry is the following: <m>\col{A}=\RR^n</m> if and only if the columns of <m>A</m> span an <m>n</m>-dimensional figure in <m>\RR^n</m>.
      To get an idea of why this should be true, consider a <m>2\times2</m> matrix <m>A</m> whose columns determine a parallelogram (as opposed to a line) in <m>\RR^2</m> such as
      <me>A = \mqty[1 \amp 1 \\ 0 \amp 2].</me>
      Since <m>\col{A}</m> is the set of all linear combinations of columns of <m>A</m>, and geometrically this is just the set of all points that we can reach in <m>\RR^2</m> by stretching and expanding the parallelogram determined by the columns, it follows that <m>\col{A} = \RR^2</m>.
    </p>
    <p>
      The <em>determinant</em> makes this observation precise.
      Given an <m>n\times n</m> square matrix <m>A</m>, the determinant of <m>A</m> represents the (signed) volume of the parallelepiped determined by the columns of <m>A</m>.
      If this volume is nonzero then this means that the parallelepiped must be an <m>n</m>-dimensional figure in <m>\RR^n</m> and so the column space of <m>A</m> would be all of <m>\RR^n</m>.
      In the <m>2\times2</m> case it's not too difficult to compute the determinant.
      If
      <me>A = \mqty[a \amp b \\ c \amp d],</me>
      then <m>\det(A) = \mqty|a \amp b \\ c \amp d| = ad - bc</m>.
      Note that <m>ad-bc</m> does give the area of the parallelogram determined by the columns of <m>A</m>.
      In three dimensions and higher the formula becomes more complicated and must be defined recursively.
    </p>
    <definition xml:id="definition-determinant-of-a-matrix">
      <title>Determinant of a Matrix</title>
      <idx>determinant</idx>
      <statement>
        <p>
          Let <m>A = \smqty[a_{ij}]</m> be an <m>n\times n</m> matrix.
          Let <m>A_{ij}</m> denote the <em>sub-matrix</em> of <m>A</m> obtained by removing the <m>i^\th</m> row and <m>j^\th</m> column of <m>A</m> (the same row and column containing the entry <m>a_{ij}</m>).
          Then the <term>determinant</term> of <m>A</m> is defined recursively by the formula
          <me>\det(A) = \sum_{j=1}^{n}(-1)^{1+j}a_{1j}\det(A_{1j}).</me>
          <aside>
            <p>
              We'll see later that we can expand along any row or column, but for now we'll stick to the first row.
            </p>
          </aside>
        </p>
      </statement>
    </definition>
    <example xml:id="example-computing-a-determinant">
      <title>Computing a Determinant</title>
      <statement>
        <p>
          Let
          <me>A = \mqty[2 \amp -6 \amp 4 \\ 3 \amp 5 \amp -2 \\ 1 \amp 6 \amp 3].</me>
          Find <m>\det(A)</m>.
        </p>
      </statement>
      <solution>
        <p>
          The formula in <xref ref="definition-determinant-of-a-matrix" text="type-global" /> states that
          <me>\det(A) = 2\mqty|5 \amp -2 \\ 6 \amp 3| - (-6)\mqty|3 \amp -2 \\ 1 \amp 3| + 4\mqty|3 \amp 5 \\ 1 \amp 6|</me>
          which simplifies to <m>172</m>.
          This can be confirmed in the Octave cell below.
        </p>
      </solution>
    </example>
    <sage language="octave">
      <input>
        A = [2, -6, 4; 3, 5, -2; 1, 6, 3]
        det(A)
      </input>
    </sage>
    <p>
      When computing determinants by hand, it's often useful to expand along the row or column containing the most zeros instead of just the first row.
      As long as we're careful about signs, the next result says this is permissible.
    </p>
    <theorem xml:id="theorem-cofactor-expansion">
      <title>Cofactor Expansion</title>
      <idx>
        <h>determinant</h><h>cofactor expansion</h>
      </idx>
      <statement>
        <p>
          Let <m>A=[a_{ij}]</m> be an <m>n\times n</m> matrix and define <m>A_{ij}</m> as in <xref ref="definition-determinant-of-a-matrix" text="type-global" />.
          Then
          <me>\det(A) = \sum_{i=1}^{n}(-1)^{i+j}a_{ij}\det(A_{ij}) = \sum_{j=1}^{n}(-1)^{i+j}a_{ij}\det(A_{ij}).</me>
        </p>
      </statement>
    </theorem>
    <example xml:id="example-computing-a-determinant-with-a-cofactor-expansion">
      <title>Computing a Determinant with a Cofactor Expansion</title>
      <statement>
        <p>
          Let
          <me>A = \mqty[1 \amp -2 \amp 5 \amp 2 \\ 0 \amp 0 \amp 3 \amp 0 \\ 2 \amp -6 \amp -7 \amp 5 \\ 5 \amp 0 \amp 4 \amp 4].</me>
          Find <m>\det(A)</m>.
        </p>
      </statement>
      <solution>
        <p>
          We can save some work by expanding along the second row to take advantage of the zeros that appear.
          Doing so, we get
          <me>\det(A) = -3\mqty|1 \amp -2 \amp 2 \\ 2 \amp -6 \amp 5 \\ 5 \amp 0 \amp 4| = -3\qty(5\mqty|-2 \amp 2\\ -6 \amp 5| + 4\mqty|1 \amp -2 \\ 2 \amp -6|)</me>
        </p>
      </solution>
    </example>
    <p>
      Computing determinants becomes very simple when working with <em>triangular matrices</em>.
    </p>
    <definition xml:id="definition-triangular-matrices">
      <title>Triangular Matrices</title>
      <statement>
        <p>
          A matrix <m>A</m> is <term>lower</term> (respectively, <term>upper</term>) <term>triangular</term> is all of the entries above (respectively, below) the main diagonal are <m>0</m>.
          A matrix is <term>triangular</term> if it is lower triangular or upper triangular.
        </p>
      </statement>
    </definition>
    <example xml:id="example-computing-the-determinant-of-a-triangular-matrix">
      <title>Computing the Determinant of a Triangular Matrix</title>
      <statement>
        <p>
          Let
          <me>A = \mqty[1 \amp 2 \amp 3 \\ 0 \amp 4 \amp 5 \\ 0 \amp 0 \amp -2].</me>
          Find <m>\det(A)</m>.
        </p>
      </statement>
      <solution>
        <p>
          Using appropriate cofactor expansions, we see that
          <me>\det(A) = 1\cdot4\cdot(-2).</me>
        </p>
      </solution>
    </example>
    <theorem xml:id="theorem-determinants-of-triangular-matrices">
      <title>Determinants of Triangular Matrices</title>
      <idx>
        <h>determinant</h>
        <h>triangular matrix</h>
      </idx>
      <statement>
        <p>
          Let <m>A</m> be an <m>n\times n</m> triangular matrix.
          Then <m>\det(A)</m> is just the product of its diagonal entries:
          <me>\det(A) = \prod_{i=1}^{N}a_{ii}.</me>
        </p>
      </statement>
    </theorem>
  </section>
</chapter>
