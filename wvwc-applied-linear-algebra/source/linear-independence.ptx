<?xml version="1.0" encoding="UTF-8" ?>

<!--********************************************************************

*********************************************************************-->
<!-- This file was originally part of the book     -->
<!-- (as copied on 2015/07/12)                     -->
<!--                                               -->
<!--   Abstract Algebra: Theory and Applications   -->
<!--                                               -->
<!-- Copyright (C) 1997-2014  Thomas W. Judson     -->

<chapter xml:id="chapter-linear-independence" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Linear Independence</title>
  <section xml:id="section-linear-independence-dependence">
    <title>Linear Independence and Dependence</title>
    <p>
      One of the fundamental concepts in linear algebra is that of a <em>linear combination</em> of several vectors.
      Given a collection <m>\set{\vecmm{x}_i}</m>, a linear combination of these vectors is a sum of the form
      <me>\sum c_i \vecmm{x}_i</me>.
    </p>
    <example xml:id="example-finding-linear-combinations">
      <title>Finding linear combinations</title>
      <statement>
        <p>
          Let
          <me>\vecmm{x}_1 = \mqty[1 \amp 2]^{T}, \vecmm{x}_2 = \mqty[-1 \amp 1]^{T}\text{ and }\vecmm{x}_3 = \mqty[0 \amp 1]^{T}</me>.
          Compute
          <me>2\vecmm{x}_1 - \vecmm{x}_2 + 2 \vecmm{x}_3, 4\vecmm{x}_3 - \vecmm{x}_2</me>.
        </p>
      </statement>
    </example>
    <p>
      In the last example, we saw that <m>\vecmm{x}_1</m> itself can be written as a linear combination of the other two vectors.
      Equivalently,
      <me>-\vecmm{x}_1 - \vecmm{x}_2 + 4\vecmm{x}_3 = \vecmm{0}</me>.
      This is an example of <term>linear dependence</term>.
      If a set of vectors is not linearly dependent, we say that it is <term>linearly independent</term>.
    </p>
    <example xml:id="example-linearly-independent-set">
      <title>Linearly independent set</title>
      <statement>
        <p>
          Determine if <m>\smqty[2\\0], \smqty[1\\1]\in\RR^2</m> forms a linearly independent collection of vectors.
        </p>
      </statement>
    </example>
    <p>
      Linearly independent collections have the useful property that linear combinations of vectors in the collection are unique.
      On the other hand, any vector that can be written as the linear combination of linearly dependent vectors can be written as infinitely many linear combinations of those same vectors.
      This property makes linearly independent sets useful for coordinate systems.
    </p>
    <p>
      Determining whether or not a collection of vectors is linearly independent can be done in several ways, but the most fundamental method is <em>row reduction</em>.
      This involves placing the vectors as columns of a single matrix and then reducing the matrix into an echelon form.
      Columns in the echelon form that have a leading <m>1</m> correspond to a linearly independent subset of the vectors.
      If we go a step further and find the reduced echelon form, we can also read off how to write vectors in the collection as a linear combination of the others.
    </p>
    <example xml:id="example-echelon-forms-and-linear-independence">
      <title>Echelon forms and linear independence</title>
      <statement>
        <p>
          Let
          <me>\vecmm{x}_1 = \mqty[1 \\ -2 \\ 3], \vecmm{x}_2 = \mqty[0 \\ 1 \\ 0], \vecmm{x}_3 = \mqty[2 \\ -1 \\ 6], \vecmm{x}_4 = \mqty[-1 \\ -2 \\ 0]</me>.
          To determine if these vectors are linearly independent and what linear dependence relations exist among these vectors, we use Octave to compute the reduced row echelon form with the <c>rref</c> command.
        </p>
        <sage language="octave">
          <input>
            % create vectors
            x1 = [1; -2; 3];
            x2 = [0; 1; 0];
            x3 = [2; -1; 6];
            x4 = [-1; -2; 0];

            % stack vectors into single matrix
            A = [x1, x2, x3, x4];

            % compute RREF of A
            rref(A)
          </input>
          <output>
            ans =

                 1     0     2     0
                 0     1     3     0
                 0     0     0     1
           </output>
         </sage>
         <p>
          The RREF of our matrix shows that the first, second and fourth vectors form a linearly independent set.
          Furthermore, <m>\vecmm{x}_3 = 2\vecmm{x}_1 + 3\vecmm{x}_2.</m>
          Hence, the first three vectors form a dependent set.
        </p>
      </statement>
    </example>
  </section>
  <section xml:id="section-basis">
    <title>Basis</title>
    <p>
      The size of a linearly independent set is limited by the dimension of a space.
      In particular, any linearly independent set of <m>n</m>-vectors has at most <m>n</m> vectors.
      Conversely, any collection of <m>n</m>-vectors with more than <m>n</m> vectors in the set must be linearly dependent.
    </p>
    <p>
      Linearly independent sets that are as large as possible are special in linear algebra and are called <term>bases</term>.
      The defining characteristic of a basis is the following <em>expansion property</em>: if <m>\vecmm{x}\in\RR^n</m> and if <m>\mathcal{B} = \set{\vecmm{b}_i}</m> is a basis for <m>\RR^n</m>, then there exists exactly one linear combination of the basis vectors that equals <m>\vecmm{x}</m>.
    </p>
    <example xml:id="example-bases-in-">
      <title>Bases in <m>\RR^3</m></title>
      <statement>
        <p>
          One example of a basis in <m>\RR^3</m> is the standard basis.
          Another example is the collection <m>\vecmm{x}_1, \vecmm{x}_2, \vecmm{x}_4</m> from <xref ref="example-echelon-forms-and-linear-independence" text="type-global" />.
        </p>
      </statement>
    </example>
    <p>
      A basis is essentially a coordinate system to work with.
      We will usually work with the coordinate system determined by the standard basis, but other coordinate systems are useful as well.
      In particular, it is often worthwhile to choose a basis that leads to <em>sparse coordinates</em>.
    </p>
    <example xml:id="example-images-and-bases">
      <title>Images and bases</title>
      <statement>
        <p>
          Consider the problem of representing a <m>3\times 3</m> grayscale image using vectors.
          This can be done using <m>9</m>-vectors whose entries run from <m>0</m> (black) to <m>1</m> (white).
          Let's also suppose that the images we are representing are known to have a consistent cross pattern through the middle.
          The color of the cross varies, but it always appears in our images.
          In this case a useful basis for representing these images might be
          <me>\vecmm{b}_1 = \mqty[0\\1\\0\\1\\1\\1\\0\\1\\0], \vecmm{b}_2 = \mqty[1\\0\\0\\0\\0\\0\\0\\0\\0], \vecmm{b}_3 = \mqty[0\\0\\1\\0\\0\\0\\0\\0\\0], \vecmm{b}_4 = \mqty[0\\0\\0\\0\\0\\0\\1\\0\\0], \vecmm{b}_5 = \mqty[0\\0\\0\\0\\0\\0\\0\\0\\1],</me>
        </p>
        <p>
          For example, consider the following image <m>A</m> rendered using Octave:
        </p>
        <sage language="octave">
          <input>
            A = [0.2, 0.76, 0.4, 0.76, 0.76, 0.76, 0.1, 0.76, 0.2];
            imshow(reshape(A, [3,3]))
          </input>
        </sage>
        <p>
          This image has nine pixels and would therefore seem to require nine separate values to transmit correctly.
          However, if we use our basis vectors to express <m>A</m> then we get
          <me>A = 0.76\vecmm{b}_1 + 0.2\vecmm{b}_2 + 0.4\vecmm{b}_3 + 0.1\vecmm{b}_4 + 0.2\vecmm{b}_5</me>.
        </p>
      </statement>
    </example>
    <conclusion>
      <p>
        SUGGESTED PROBLEMS: 5.2
      </p>
    </conclusion>
  </section>
  <section xml:id="section-orthonormal-vectors">
    <title>Orthonormal Vectors</title>
    <p>
      A collection of vectors is <term>orthonormal</term> if it is both orthogonal and unit-normed.
      The usual example of an orthonormal set is the collection of coordinate unit vectors in <m>\RR^n</m> <m>\set{\vecm{e}_i}</m>, but these are not the only ones.
    </p>
    <example xml:id="example-testing-orthonormality">
      <title>Testing orthonormality</title>
      <statement>
        <p>
          Determine if the given vectors are orthonormal.
        </p>
      </statement>
      <solution>
        <p>
        </p>
      </solution>
    </example>
    <p>
      Orthogonal collections of nonzero vectors are always linearly independent:
      if <m>\sum c_i\vecm{x}_i = \vecm{0}</m> for orthogonal vectors, then taking the inner product of this sum with the vector <m>\vecm{x}_j</m> gives <m>c_j = 0</m>.
    </p>
    <p>
      Bases that are also orthonormal (ONBs) have special importance in linear algebra:
      if <m>\vecm{x} = \sum c_i\vecm{x}_i</m> for orthonormal vectors, then <m>c_i = \abrackets{\vecm{x},\vecm{x}_i}</m>.
      Since a basis can be used to express any vector in our space, the previous equation makes it simple to find the corresponding coefficients to expand an arbitrary vector <m>\vecm{x}</m> in terms of an ONB.
    </p>
    <example xml:id="example-expansion-in-terms-of-an-onb">
      <title>Expansion in terms of an ONB</title>
      <statement>
        <p>
        </p>
      </statement>
      <solution>
        <p>
        </p>
      </solution>
    </example>
  </section>
  <section xml:id="section-gram-schmidt-orthogonalization">
    <title>Gram-Schmidt Orthogonalization</title>
    <p>
      Given a linearly independent set <m>\{\vecm{v}_i\}</m>, we often want to <em>orthogonalize</em> it to make certain computations easier to perform.
      This can be done (at least theoretically) by using the Gram-Schmidt Algorithm.
      The algorithm works be successively subtracting out parallel components of vectors and normalizing what remains to produce an orthonormal set.
      The parallel components are themselves determined by the inner product: given two nonzero vectors <m>\vecm{u},\vecm{v}</m> in <m>\RR^n</m>, we can always write
      <me>\vecm{u} = \vecm{v}_{\parallel} + \vecm{v}_{\perp}</me>
      where
      <md>
        <mrow>\vecm{v}_{\parallel} \amp = \frac{\vecm{u}^{T}\vecm{v}}{\norm{\vecm{v}}^2}\vecm{v} </mrow>
        <mrow>\vecm{v}_{\perp} \amp = \vecm{u} - \vecm{v}_{\parallel} </mrow>
      </md>.
    </p>
    <p>
      Given <m>k</m> vectors <m>\{\vecm{v}_1,\ldots, \vecm{v}_k\}</m>, the algorithm will either produce <m>k</m> nonzero orthogonal vectors or it will terminate before the <m>k</m>th step and produce the zero vector as one of its outputs.
      This second option occurs precisely when the original collection is linearly dependent.
      Hence, the Gram-Schmidt algorithm will always turn a collection of linearly independent vectors into an orthogonal (and therefore linearly independent) collection of the same size.
      In particular, the algorithm converts bases into ONBs.
    </p>
    <example xml:id="example-using-the-gram-schmidt-algorithm-to-construct-an-onb">
      <title>Using the Gram-Schmidt algorithm to construct an ONB</title>
      <statement>
        <p>
          Apply the Gram-Schmidt algorithm to the vectors
          <me>\vecm{v}_1 = \mqty[1\\-1\\0], \vecm{v}_2 = \mqty[0 \\ 0 \\ 2],\vecm{v}_3 = \mqty[1\\0\\1]</me>.
        </p>
      </statement>
      <solution>
        <p>
          We start by setting <m>\vecm{q}_1 = \frac{\vecm{v}_1}{\norm{\vecm{v}_1}}</m>:
          <me>\vecm{q}_1 = \frac{1}{\sqrt{2}}\mqty[1\\-1\\0]</me>.
          Now we orthogonalize the set <m>\{\vecm{q}_1, \vecm{v}_2\}</m> by replacing <m>\vecm{v}_2</m> with <m>\vecm{\tilde{q}}_2=\vecm{v}_2 - (\vecm{q}_1^{T}\vecm{v}_2)\vecm{u}_2</m> and then normalizing the result:
          <md>
            <mrow>\vecm{\tilde{q}}_2 = \vecm{v}_2 - (\vecm{q}_1^{T}\vecm{v}_2)\vecm{q}_1 \amp = \mqty[0\\0\\1] </mrow>
            <mrow>\vecm{q}_2 \amp = \mqty[0\\0\\1] </mrow>
          </md>.
          Finally, we orthogonalize the set <m>\{\vecm{q}_1, \vecm{q}_2, \vecm{v}_3\}</m>:
          <md>
            <mrow>\vecm{\tilde{q}}_3 = \vecm{v}_3 - (\vecm{q}_1^{T}\vecm{v}_3)\vecm{q}_1 - (\vecm{q}_2^{T}\vecm{v}_3)\vecm{q}_2 \amp =  \mqty[\frac{1}{2} \\ \frac{1}{2} \\ 0]</mrow>
            <mrow>\vecm{q}_3 \amp = \frac{1}{\sqrt{2}}\mqty[1 \\ 1 \\ 0] </mrow>
          </md>.
          The resulting set <m>\{\vecm{q}_1, \vecm{q}_2, \vecm{q}_3\}</m> is an orthonormal set (in fact, an ONB) in <m>\RR^3</m>.
        </p>
      </solution>
    </example>
    <example xml:id="example-gram-schmidt-and-linear-dependence">
      <title>Gram-Schmidt and linear dependence</title>
      <statement>
        <p>
          Apply the Gram-Schmidt algorithm to the vectors
          <me>\mqty[1 \\ -1 \\ 0], \mqty[0\\0\\1], \mqty[2\\-2\\1], \mqty[1\\0\\2]</me>.
        </p>
      </statement>
      <solution>
        <p>
          If we apply the Gram-Schmidt process to this collection of vectors we will eventually get <m>\vecm{v}_3 = \vecm{0}</m>.
          This is because the first three vectors in our collection are linearly dependent.
          At this point the Gram-Schmidt produces no more useful results.
          If we want to obtain a linearly independent orthonormal set, we will need to remove <m>\vecm{b}_3</m> from our collection.
        </p>
      </solution>
    </example>
    <subsection xml:id="subsection-modified-gram-schmidt-algorithm">
      <title>The modified Gram-Schmidt algorithm.</title>
      <p>
        Theoretically, the Gram-Schmidt algorithm is guaranteed to produce an orthonormal set as long as the input is a linearly independent set.
        In practice, however, roundoff errors in computer computations can produce vectors that are no longer orthogonal.
        To combat this, the <em>modified Gram-Schmidt algorithm</em> (MGS) can be used.
        The MGS process differs from the typical Gram-Schmidt process as follows: once the vector <m>\vecm{q}_j</m> is produced in step <m>j</m> of the algorithm, all subsequent vectors <m>\vecm{v}_k, j+1\leq k\leq n</m> are replaced with <m>\vecm{v}_k - (\vecm{q}_j^{T}\vecm{v}_k)\vecm{q}_j</m>.
        That is, we continually update the remaining vectors in our collection to be orthogonal to our current collection of orthonormal vectors.
      </p>
      <example xml:id="example-using-the-mgs-algorithm-to-construct-an-onb">
        <title>Using the modified Gram-Schmidt algorithm to construct an ONB</title>
        <statement>
          <p>
            Apply the modified Gram-Schmidt algorithm to the vectors
            <me>\vecm{v}_1 = \mqty[1\\-1\\0], \vecm{v}_2 = \mqty[0 \\ 0 \\ 2],\vecm{v}_3 = \mqty[1\\0\\1]</me>.
          </p>
        </statement>
        <solution>
          <p>
            As before, we start by setting <m>\vecm{q}_1 = \frac{\vecm{v}_1}{\norm{\vecm{v}_1}}</m>:
            <me>\vecm{q}_1 = \frac{1}{\sqrt{2}}\mqty[1\\-1\\0]</me>.
            Now we orthogonalize the set <m>\{\vecm{q}_1, \vecm{v}_2, \vecm{v}_3\}</m> with respect to <m>\vecm{q}_1</m>:
            <md>
              <mrow>\vecm{\tilde{q}}_2 = \vecm{v}_2 - (\vecm{q}_1^{T}\vecm{v}_2)\vecm{q}_1 \amp = \mqty[0\\0\\1] </mrow>
              <mrow>\vecm{\tilde{v}}_3 = \vecm{v}_3 - (\vecm{q}_1^{T}\vecm{v}_3)\vecm{q}_2 \amp = \mqty[\frac{1}{2}\\\frac{1}{2}\\1] </mrow>
            </md>.
            This produces <m>\vecm{q}_2 = \smqty[0\\0\\1]</m> as before, but now <m>\vecm{v}_3</m> has been replaced with a vector that is also orthogonal to <m>\vecm{q}_1</m>.
            The final step is to orthogonalize the set <m>\{\vecm{q}_1, \vecm{q}_2, \vecm{\tilde{v}}_3\}</m>:
            <md>
              <mrow>\vecm{\tilde{q}}_3 = \vecm{\tilde{v}}_3 - (\vecm{q}_2^{T}\vecm{\tilde{v}}_3)\vecm{q}_2 \amp =  \mqty[\frac{1}{2} \\ \frac{1}{2} \\ 0]</mrow>
              <mrow>\vecm{q}_3 \amp = \frac{1}{\sqrt{2}}\mqty[1 \\ 1 \\ 0] </mrow>
            </md>.
            Therefore, we have once again obtained an ONB from the MGS algorithm.
          </p>
        </solution>
      </example>
      <example xml:id="example-comparing-classical-gram-schmidt-and-modified-gram-schmidt-algorithms">
        <title>Comparing the classical Gram-Schmidt and modified Gram-Schmidt algorithms</title>
        <statement>
          <p>
            Apply the different Gram-Schmidt algorithms to the collection
            <me>\vecm{v}_1 = \mqty[1\\\varepsilon\\0\\0], \vecm{v}_2 = \mqty[1\\0\\\varepsilon\\0], \vecm{v}_3 = \mqty[1\\0\\0\\\varepsilon]</me>
            using the approximation <m>1+\varepsilon^2 = 1</m> to simulate what happens if these algorithms are applied with finite precision arithmetic.
          </p>
        </statement>
        <solution>
          <p>
            <!-- TODO: Finish example! -->
          </p>
        </solution>
      </example>
    </subsection>
  </section>
</chapter>
