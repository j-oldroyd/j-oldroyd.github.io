<?xml version="1.0" encoding="UTF-8" ?>

<!--********************************************************************

*********************************************************************-->
<!-- This file was originally part of the book     -->
<!-- (as copied on 2015/07/12)                     -->
<!--                                               -->
<!--   Abstract Algebra: Theory and Applications   -->
<!--                                               -->
<!-- Copyright (C) 1997-2014  Thomas W. Judson     -->

<chapter xml:id="chapter-matrix-inverses" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Matrix Inverses</title>
  <section xml:id="section-left-and-right-inverses">
    <title>Left and right inverses</title>
    <p>
      A <term>left inverse</term> of <m>A</m> is a matrix <m>X</m> such that <m>XA = I</m>.
      If <m>A</m> is <m>m\times n</m>, then <m>X</m> will be <m>n\times m</m> and so <m>I</m> is <m>n\times n</m>.
    </p>
    <example xml:id="example-left-inverses" label="example-left-inverses">
      <title>Left inverses</title>
      <statement>
        <p>
          Let
          <me>A = \mqty[-3 \amp -4 \\ 4 \amp 6 \\ 1 \amp 1], B = \frac{1}{9}\mqty[-11 \amp -10 \amp 16 \\ 7 \amp 8 \amp -11], C = \frac{1}{2}\mqty[0 \amp -1 \amp 6 \\ 0 \amp 1\amp -4]</me>.
          Is either <m>B,C</m> a left inverse of <m>A</m>?
        </p>
      </statement>
    </example>
    <p>
      A matrix is left-invertible only if its columns are linearly independent:
      <me>\vec{0} = A\vec{x} \implies \vec{x} = X\vec{0} = \vec{0}</me>.
      The converse is also true.
    </p>
    <example xml:id="example-determining-if-a-matrix-is-left-invertible" label="example-determining-if-a-matrix-is-left-invertible">
      <title>Determining if a matrix is left-invertible</title>
      <statement>
        <p>
          Let
          <me>A = \mqty[3 \amp 2 \amp -1 \\ 0 \amp 2 \amp 5]</me>.
          Is <m>A</m> left-invertible?
        </p>
      </statement>
    </example>
    <p>
      If <m>X</m> is a left inverse of <m>A</m>, then <m>A\vec{x} = \vec{b}</m> has a solution only if <m>\vec{x} = X\vec{b}</m>.
      If this is not a solution, then the system is inconsistent.
    </p>
    <p>
      A <term>right-inverse</term> of <m>A</m> is a matrix <m>Y</m> such that <m>AY = I</m>.
      A matrix is right invertible only if its rows are linearly independent, equivalently, only if <m>A^T</m> is left-invertible.
    </p>
  </section>
  <section xml:id="section-matrix-inverses">
    <title>Matrix inverses</title>
    <p>
      Consider the equation <m>75x = 2</m>.
      We can solve this quite easily for <m>x</m> by dividing both sides by <m>75</m>, or equivalently, multiplying both sides of the equation by <m>frac{1}{75} = 75^{-1}</m>.
      <m>75^{-1}</m> is the <em>multiplicative inverse</em> of the number <m>75</m>, and so when multiplied to it we are left with only the number <m>1</m>.
      We want to do the same with the matrix equation <m>A\vb{x} = \vb{b}</m>; that is, we want to find an <em>inverse matrix</em> <m>A^{-1}</m> that, when multiplied to <m>A</m>, leaves only the identity matrix.
    </p>
    <definition xml:id="definition-invertible-matrices">
      <title>Invertible matrices</title>
      <idx><h>matrices</h><h>invertible</h></idx>
      <statement>
        <p>
          An <m>n\times n</m> matrix <m>A</m> is said to be <term>invertible</term> (or <term>nonsingular</term>) if there exists a matrix <m>A^{-1}</m> such that <m>A^{-1}A = AA^{-1} = I_{n}</m>. We call <m>A^{-1}</m> the <term>inverse</term> of <m>A</m>.

          If a matrix is <em>not</em> invertible, then we say that it is <term>singular</term>.
        </p>
      </statement>
    </definition>
    <p>
      Note that if <m>A</m> is a square matrix and <m>C</m> is another square matrix such that either <m>AC = I</m> or <m>CA = I</m>, then <m>C = A^{-1}</m>.
    </p>
    <example xml:id="example-confirming-a-matrix-inverse">
      <title>Confirming a matrix inverse</title>
      <statement>
        <p>
          Let
          <me>A = \mqty[1 &amp; 0 &amp; -2 \\ -3 &amp; 1 &amp; 4 \\ 2 &amp; -3 &amp; 4].</me>
        </p>
        <ol>
          <li>Show that the matrix <me>C = \mqty[8 &amp; 3 &amp; 1 \\ 10 &amp; 4 &amp; 1 \\ \frac{7}{2} &amp; \frac{3}{2} &amp; \frac{1}{2}]</me> is the inverse of <m>A</m>.</li>
          <li>Let <me>\vb{b} = \mqty[1 \\ 1 \\ 1].</me> Solve <m>A\vb{x} = \vb{b}</m>.</li>
        </ol>
      </statement>
      <solution>
        <ol>
          <li>All we need to do is to show that <m>AC = I</m>. This can be done quickly using Octave as in the code cell below.</li>
          <li>The solution is <m>\vb{x} = A^{-1}\vb{b}</m>. Given that we now know <m>A^{-1}</m>, we can solve this quickly.</li>
        </ol>
      </solution>
    </example>
    <sage language="octave">
      <input>
        A = [1, 0, -2; -3, 1, 4; 2, -3, 4];
        C = [8,3,1; 10,4,1; 7/2, 3/2, 1/2];

        A*C % identity matrix

        b = [1;1;1]
        x = C*b % x = inv(A)*b is the solution
      </input>
    </sage>
    <example>
      <title>Inverse of an orthogonal matrix</title>
      <statement>
        <p>
          Let <m>U</m> be an orthogonal matrix.
          What is <m>U^{-1}</m>?
        </p>
      </statement>
      <solution>
        <p>
          <m>U^{-1} = U^{T}</m>.
          So it is <em>very</em> easy to find the inverse of an orthogonal matrix.
        </p>
      </solution>
    </example>
    <p>
      An important property about determinants is that they say precisely when a matrix is invertible.
      If <m>A</m> is an <m>n\times n</m> matrix, then <m>A</m> has an inverse if and only if <m>\det A \neq 0</m>.
    </p>
    <theorem xml:id="theorem-invertibility-and-solutions">
      <title>Invertibility and solutions of systems</title>
      <idx>
        <h>linear systems</h>
        <h>invertibility</h>
      </idx>
      <statement>
        <p>
          Let <m>A</m> be an invertible <m>n\times n</m> matrix.
          Then for each <m>\vb{b}\in\RR^{n}</m>, the matrix equation <m>A\vb{x} = \vb{b}</m> has <em>exactly</em> one solution: <m>\vb{x} = A^{-1}\vb{b}</m>.
        </p>
      </statement>
      <proof>
        <p>
          To prove this statement we must show two things:
          <ol>
            <li>
              <p><m>A^{-1}\vb{b}</m> is a solution.</p>
            </li>
            <li>
              <p><m>A^{-1}\vb{b}</m> is the only solution.</p>
            </li>
          </ol>
        </p>
        <p>
          We start with the first item.
          To check that <m>A^{-1}\vb{b}</m> is a solution of <m>A\vb{x} = \vb{b}</m>, we just plug it in for <m>\vb{x}</m> and simplify:
          <me>A(A^{-1}\vb{b}) = I\vb{b} = \vb{b}.</me>
          Hence this is a solution.
        </p>
        <p>
          To show that this is the only solution, suppose that <m>\vb{u}</m> is some other solution of <m>A\vb{x} = \vb{b}</m>.
          We must show that <m>\vb{u} = A^{-1}\vb{b}</m>.
          Since <m>\vb{u}</m> is assumed to be a solution, we have
          <me>\vb{u} = \vb{b}\Rightarrow \vb{u} = A^{-1}\vb{b}.</me>
          Hence <m>A^{-1}\vb{b}</m> is the only solution.
        </p>
      </proof>
    </theorem>
    <subsection xml:id="subsection-computing-the-inverse-of-a-matrix">
      <title>Computing the inverse of a matrix</title>
      <p>
        For <m>2\times 2</m> matrices, we have a simple formula for the inverse.
      </p>
      <theorem xml:id="theorem-inverse-of-2-times2--matrices">
        <title>Inverse of <m>2\times2</m> matrices</title>
        <statement>
          <p>
            Let
            <me>A = \mqty[a &amp; b \\ c &amp; d].</me>
            If <m>ad-bc\neq0</m>, then
            <me>A^{-1} = \frac{1}{ad-bc}\mqty[d &amp; -b \\ -c &amp; a].</me>
          </p>
        </statement>
      </theorem>
      <p>
        The quantity <m>ad-bc</m> in <xref ref="theorem-inverse-of-2-times2--matrices" text="type-global" /> can also be recognized as the determinant of the matrix <m>A</m>.
        See <xref ref="definition-determinant-of-a-matrix" text="type-global" />.
      </p>
      <example>
        <statement>
          <p>
            Show that the system
            <md>
              <mrow>9x_{1}+3x_{2} &amp;= -9</mrow>
              <mrow>-6x_{1}-3x_{2} &amp;= 4</mrow>
            </md>
            is consistent and then find the solution.
          </p>
        </statement>
        <solution>
          <p>
            We can rewrite this as the matrix equation <m>A\vb{x} = \vb{b}</m>, where
            <me>A = \mqty[9 &amp; 3 \\ -6 &amp; -3],\qq{}\vb{x} = \mqty[x_{1} \\ x_{2}]\qq{and} \vb{b} = \mqty[-9 \\ 4].</me>
            We can show that the system is consistent by computing <m>\det A</m>: since <m>\det A = -9\neq0</m>, <m>A^{-1}</m> exists.
            And since <m>A^{-1}</m> exists, the system must be solvable.
          </p>
          <p>
            To solve it, we use the above formula to compute <m>A^{-1}</m>:
            <me>A^{-1} = -\frac{1}{9}\mqty[-3 &amp; -3 \\ 6 &amp; 9] = \mqty[\frac{1}{3} &amp; \frac{1}{3} \\ -\frac{2}{3} &amp; -1 ].</me>
            So the (unique) solution is
            <me>\vb{x} = A^{-1}\vb{b} = \mqty[\frac{1}{3} &amp; \frac{1}{3} \\ -\frac{2}{3} &amp; -1 ]\mqty[-9 \\ 4].</me>
          </p>
        </solution>
      </example>
      <theorem>
        <statement>
          <p>
            Let <m>A</m> and <m>B</m> be invertible <m>n\times n</m> matrices.
            <ol>
              <li>
                <p><m>(A^{-1})^{-1} = A</m></p>
              </li>
              <li>
                <p><m>AB</m> is invertible, and <m>(AB)^{-1} = B^{-1}A^{-1}</m>.</p>
              </li>
              <li>
                <p><m>A^{T}</m> is invertible, and <m>(A^{T})^{-1} = (A^{-1})^{T}</m>.</p>
              </li>
            </ol>
          </p>
        </statement>
      </theorem>
    </subsection>
    <subsection xml:id="subsection-invertible-matrix-algorithm">
      <title>The invertible matrix algorithm</title>
      <p>
        We can now find the inverse of a <m>2\times2</m> matrix; we need to determine how to find the inverse of a larger matrix. To do this, we will use <em>elementary matrices</em>.
      </p>
      <definition xml:id="definition-elementary-matrices">
        <title>Elementary matrices</title>
        <statement>
          <p>
            An <term>elementary matrix</term> is a matrix obtained by performing a single elementary row operation on the identity matrix.
          </p>
        </statement>
      </definition>
      <example>
        <statement>
          <p>
            The matrices
            <me>E_{1} = \mqty[2 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1],E_{2} = \mqty[1 &amp; 5 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1]\text{ and } E_{3} = \mqty[0 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0]</me>
            are elementary matrices.
            The first corresponds to scaling the first row by <m>2</m>; the second corresponds to adding five times the second row to the first row; and the third corresponds to switching rows one and three.
          </p>
        </statement>
      </example>
      <p>
        The important fact about elementary matrices is that multiplying them to <em>any</em> matrix has the same effect as performing the corresponding elementary row operation on the matrix.
      </p>
      <example>
        <statement>
          <p>
            Let
            <me>A = \mqty[1 &amp; 2 &amp; 9 \\ 0 &amp; 3 &amp; 3 \\ 4 &amp; 4 &amp; 1].</me>
            Use elementary matrices to perform the following row operations:
            <ol>
              <li>
                <p>Add two times row three to row two.</p>
              </li>
              <li>
                <p>Scale row three by <m>-3</m>.</p>
              </li>
              <li>
                <p>Swap row two with row one and then add five times row three to row one.</p>
              </li>
            </ol>
          </p>
        </statement>
        <solution>
          <p>
            For each case, we only need to determine the elementary matrix corresponding to each row operation.
            The elementary matrix for the first operation is
            <me>E_{1} = \mqty[1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 2 \\ 0 &amp; 0 &amp; 1].</me>
            To perform this operation on <m>A</m>, we just multiply <m>E_{1}</m> and <m>A</m>:
            <md>
              <mrow>A &amp;\rowop{2R_{3}+R_{2}} E_{1}A </mrow>
              <mrow> &amp;= \mqty[1 &amp; 2 &amp; 9 \\ 8 &amp; 11 &amp; 5 \\ 4 &amp; 4 &amp; 1] </mrow>
            </md>
            which matches with the matrix we would have obtained just using a row operation.
          </p>
          <p>
            The elementary matrix we need for the next operation is
            <me>E_{2} = \mqty[1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; -3]</me>
            and so
            <me>A\rowop{-3R_{3}} E_{2}A.</me>
          </p>
          <p>
            Finally, we have two elementary row operations here, so we can't just use a single elementary matrix. We'll need to use two; one for each row operation:
            <me>E_{3} = \mqty[0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1]\qq{and} E_{4} = \mqty[1 &amp; 0 &amp; 5 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1].</me>
            So
            <me>A\rowop[R_{1}\leftrightarrow R_{2}]{5R_{3}+R_{1}} E_{4}E_{3}A.</me>
          </p>
        </solution>
      </example>
      <p>
        So row operations on a matrix can be viewed as multiplications by elementary matrices.
        And since row operations are invertible, elementary matrices are invertible as well.
        To find the inverse of an elementary matrix <m>E</m>, just write down the elementary matrix corresponding to the row operation that transforms <m>E</m> back into <m>I</m>.
      </p>
      <example>
        <title>Inverse of an elementary matrix</title>
        <statement>
          <p>
            Let <m>E_{1},E_{2}</m> and <m>E_{4}</m> be as above. Find the inverse of each matrix.
          </p>
        </statement>
        <solution>
          <p>
            We have
            <me>E_{1}^{-1} = \mqty[1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; -2 \\ 0 &amp; 0 &amp; 1],\qq{}E_{2}^{-1} = \mqty[1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; -\frac{1}{3}]\qq{and}E_{4}^{-1} = \mqty[1 &amp; 0 &amp; -5 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1].</me>
          </p>
        </solution>
      </example>
      <theorem xml:id="theorem-invertible-matrix-algorithm">
        <title>Invertible matrix algorithm</title>
        <idx>
          <h>matrices</h>
          <h>invertible</h>
          <h>algorithm</h>
        </idx>
        <statement>
          <p>
            Let <m>A</m> be an <m>n\times n</m> matrix.
            If <m>A\sim I</m>, then <m>A</m> is invertible.
          </p>
        </statement>
        <proof>
          <p>
            Suppose that <m>A</m> is row equivalent to the identity matrix <m>I</m>.
            Then we can find elementary matrices <m>E_{1},E_{2},\ldots,E_{p}</m> such that
            <me>I = E_{p}E_{p-1}\cdots E_{2}E_{1}A.</me>
            Since elementary matrices are invertible, their product must be as well.
            So we can write
            <me>(E_{p}\cdots E_{1})^{-1} = A.</me>
            Since <m>A</m> is the inverse of an invertible matrix, it must itself be invertible and furthermore
            <me>A^{-1} = E_{p}\cdots E_{1}.</me>
          </p>
        </proof>
      </theorem>
      <p>
        The above theorem tells us that the sequence of row operations that reduces <m>A</m> to <m>I</m> also turns <m>I</m> into <m>A^{-1}</m>.
        This gives us an algorithm for finding the inverse of a matrix. We show this with an example.
      </p>
      <example>
        <statement>
          <p>
            Let
            <me>A = \mqty[-1 &amp; -7 &amp; -3 \\ 2 &amp; 15 &amp; 6 \\ 1 &amp; 3 &amp; 2].</me>
            Compute <m>A^{-1}</m>.
          </p>
        </statement>
        <solution>
          <p>
            We set up the augmented matrix <m>\mqty[A&amp; I]</m>. The algorithm works by finding the reduced echelon form; the resulting augmented matrix is then <m>\mqty[I&amp; A^{-1}]</m>.
            <md>
              <mrow>\mqty[-1 &amp; -7 &amp; -3 &amp; 1 &amp; 0 &amp; 0 \\ 2 &amp; 15 &amp; 6 &amp; 0 &amp; 1 &amp; 0 \\ 1 &amp; 3 &amp; 2 &amp; 0 &amp; 0 &amp; 1] &amp;\sim\mqty[1 &amp; 7 &amp; 3 &amp; -1 &amp; 0 &amp; 0 \\ 2 &amp; 15 &amp; 6 &amp; 0 &amp; 1 &amp; 0 \\ 1 &amp; 3 &amp; 2 &amp; 0 &amp; 0 &amp; 1]</mrow>
              <mrow>&amp;\rowop[-2R_{1}+R_{2}]{-R_{1}+R_{3}}\mqty[1 &amp; 7 &amp; 3 &amp; -1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 2 &amp; 1 &amp; 0 \\ 0 &amp; -4 &amp; -1 &amp; 1 &amp; 0 &amp; 1]</mrow>
              <mrow>&amp;\rowop{4R_{2}+R_{3}}\mqty[1 &amp; 7 &amp; 3 &amp; -1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 2 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; -1 &amp; 9 &amp; 4 &amp; 1]</mrow>
              <mrow>&amp;\rowop{3R_{3}+R_{1}}\mqty[1 &amp; 7 &amp; 0 &amp; 26 &amp; 12 &amp; 3 \\ 0 &amp; 1 &amp; 0 &amp; 2 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; -1 &amp; 9 &amp; 4 &amp; 1]</mrow>
              <mrow>&amp;\rowop[-7R_{2}+R_{1}]{-R_{3}}\mqty[1 &amp; 0 &amp; 0 &amp; 12 &amp; 5 &amp; 3 \\ 0 &amp; 1 &amp; 0 &amp; 2 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; -9 &amp; -4 &amp; -1]</mrow>
            </md>
          </p>
          <p>
            So
            <me>A^{-1} = \mqty[12 &amp; 5 &amp; 3 \\ 2 &amp; 1 &amp; 0 \\ -9 &amp; -4 &amp; -1].</me>
          </p>
        </solution>
      </example>
      <example>
        <statement>
          <p>
            A square matrix <m>A</m> of size <m>10\times10</m>.
            Suppose that <m>A</m> has rank <m>9</m>.
            Is <m>A</m> invertible?
          </p>
        </statement>
        <solution>
          <p>
            No! This is because <m>A</m> does not have a pivot in each row (since <m>\rank A = 9</m>, <m>A</m> only has <m>9</m> pivots).
            Therefore we can't row reduce <m>A</m> to get <m>I</m>.
            Since <m>A</m> is not row equivalent to the identity matrix, <m>A</m> cannot be invertible.
          </p>
        </solution>
      </example>
    </subsection>
  </section>
</chapter>
